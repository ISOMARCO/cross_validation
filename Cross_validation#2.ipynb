{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.714109Z",
     "start_time": "2023-09-28T13:37:37.899400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan_prediction.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.719838Z",
     "start_time": "2023-09-28T13:37:39.346777Z"
    }
   },
   "id": "567bf935cc48a91"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "      Loan_ID  Gender Married Dependents     Education Self_Employed  \\\n0    LP001002    Male      No          0      Graduate            No   \n1    LP001003    Male     Yes          1      Graduate            No   \n2    LP001005    Male     Yes          0      Graduate           Yes   \n3    LP001006    Male     Yes          0  Not Graduate            No   \n4    LP001008    Male      No          0      Graduate            No   \n..        ...     ...     ...        ...           ...           ...   \n609  LP002978  Female      No          0      Graduate            No   \n610  LP002979    Male     Yes         3+      Graduate            No   \n611  LP002983    Male     Yes          1      Graduate            No   \n612  LP002984    Male     Yes          2      Graduate            No   \n613  LP002990  Female      No          0      Graduate           Yes   \n\n     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n0               5849                0.0         NaN             360.0   \n1               4583             1508.0       128.0             360.0   \n2               3000                0.0        66.0             360.0   \n3               2583             2358.0       120.0             360.0   \n4               6000                0.0       141.0             360.0   \n..               ...                ...         ...               ...   \n609             2900                0.0        71.0             360.0   \n610             4106                0.0        40.0             180.0   \n611             8072              240.0       253.0             360.0   \n612             7583                0.0       187.0             360.0   \n613             4583                0.0       133.0             360.0   \n\n     Credit_History Property_Area Loan_Status  \n0               1.0         Urban           Y  \n1               1.0         Rural           N  \n2               1.0         Urban           Y  \n3               1.0         Urban           Y  \n4               1.0         Urban           Y  \n..              ...           ...         ...  \n609             1.0         Rural           Y  \n610             1.0         Rural           Y  \n611             1.0         Urban           Y  \n612             1.0         Urban           Y  \n613             0.0     Semiurban           N  \n\n[614 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loan_ID</th>\n      <th>Gender</th>\n      <th>Married</th>\n      <th>Dependents</th>\n      <th>Education</th>\n      <th>Self_Employed</th>\n      <th>ApplicantIncome</th>\n      <th>CoapplicantIncome</th>\n      <th>LoanAmount</th>\n      <th>Loan_Amount_Term</th>\n      <th>Credit_History</th>\n      <th>Property_Area</th>\n      <th>Loan_Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LP001002</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>5849</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LP001003</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>4583</td>\n      <td>1508.0</td>\n      <td>128.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LP001005</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>Yes</td>\n      <td>3000</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LP001006</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Not Graduate</td>\n      <td>No</td>\n      <td>2583</td>\n      <td>2358.0</td>\n      <td>120.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LP001008</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>6000</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>LP002978</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>2900</td>\n      <td>0.0</td>\n      <td>71.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>LP002979</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>3+</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>4106</td>\n      <td>0.0</td>\n      <td>40.0</td>\n      <td>180.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>LP002983</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>8072</td>\n      <td>240.0</td>\n      <td>253.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>LP002984</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>2</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>7583</td>\n      <td>0.0</td>\n      <td>187.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>LP002990</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>Yes</td>\n      <td>4583</td>\n      <td>0.0</td>\n      <td>133.0</td>\n      <td>360.0</td>\n      <td>0.0</td>\n      <td>Semiurban</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n<p>614 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.796590Z",
     "start_time": "2023-09-28T13:37:39.370799Z"
    }
   },
   "id": "191bd68a69cd234f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = df.drop('Loan_ID', axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.797494Z",
     "start_time": "2023-09-28T13:37:39.403386Z"
    }
   },
   "id": "b7cb5fa128d0823b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "     Gender Married Dependents     Education Self_Employed  ApplicantIncome  \\\n0      Male      No          0      Graduate            No             5849   \n1      Male     Yes          1      Graduate            No             4583   \n2      Male     Yes          0      Graduate           Yes             3000   \n3      Male     Yes          0  Not Graduate            No             2583   \n4      Male      No          0      Graduate            No             6000   \n..      ...     ...        ...           ...           ...              ...   \n609  Female      No          0      Graduate            No             2900   \n610    Male     Yes         3+      Graduate            No             4106   \n611    Male     Yes          1      Graduate            No             8072   \n612    Male     Yes          2      Graduate            No             7583   \n613  Female      No          0      Graduate           Yes             4583   \n\n     CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n0                  0.0         NaN             360.0             1.0   \n1               1508.0       128.0             360.0             1.0   \n2                  0.0        66.0             360.0             1.0   \n3               2358.0       120.0             360.0             1.0   \n4                  0.0       141.0             360.0             1.0   \n..                 ...         ...               ...             ...   \n609                0.0        71.0             360.0             1.0   \n610                0.0        40.0             180.0             1.0   \n611              240.0       253.0             360.0             1.0   \n612                0.0       187.0             360.0             1.0   \n613                0.0       133.0             360.0             0.0   \n\n    Property_Area Loan_Status  \n0           Urban           Y  \n1           Rural           N  \n2           Urban           Y  \n3           Urban           Y  \n4           Urban           Y  \n..            ...         ...  \n609         Rural           Y  \n610         Rural           Y  \n611         Urban           Y  \n612         Urban           Y  \n613     Semiurban           N  \n\n[614 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Married</th>\n      <th>Dependents</th>\n      <th>Education</th>\n      <th>Self_Employed</th>\n      <th>ApplicantIncome</th>\n      <th>CoapplicantIncome</th>\n      <th>LoanAmount</th>\n      <th>Loan_Amount_Term</th>\n      <th>Credit_History</th>\n      <th>Property_Area</th>\n      <th>Loan_Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Male</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>5849</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>4583</td>\n      <td>1508.0</td>\n      <td>128.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>Yes</td>\n      <td>3000</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Not Graduate</td>\n      <td>No</td>\n      <td>2583</td>\n      <td>2358.0</td>\n      <td>120.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Male</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>6000</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>Female</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>2900</td>\n      <td>0.0</td>\n      <td>71.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>3+</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>4106</td>\n      <td>0.0</td>\n      <td>40.0</td>\n      <td>180.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>8072</td>\n      <td>240.0</td>\n      <td>253.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>2</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>7583</td>\n      <td>0.0</td>\n      <td>187.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>Female</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>Yes</td>\n      <td>4583</td>\n      <td>0.0</td>\n      <td>133.0</td>\n      <td>360.0</td>\n      <td>0.0</td>\n      <td>Semiurban</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n<p>614 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.799969Z",
     "start_time": "2023-09-28T13:37:39.410162Z"
    }
   },
   "id": "af1b8637d905efa8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "label_columns = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status', 'Dependents']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.804224Z",
     "start_time": "2023-09-28T13:37:39.437355Z"
    }
   },
   "id": "5a89fb85d0923d44"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Gender               13\nMarried               3\nDependents           15\nEducation             0\nSelf_Employed        32\nApplicantIncome       0\nCoapplicantIncome     0\nLoanAmount           22\nLoan_Amount_Term     14\nCredit_History       50\nProperty_Area         0\nLoan_Status           0\ndtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.805698Z",
     "start_time": "2023-09-28T13:37:39.446240Z"
    }
   },
   "id": "89d6272342a4b19"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = df.bfill()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.806131Z",
     "start_time": "2023-09-28T13:37:39.464407Z"
    }
   },
   "id": "126b27bf858131d6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Gender               0\nMarried              0\nDependents           0\nEducation            0\nSelf_Employed        0\nApplicantIncome      0\nCoapplicantIncome    0\nLoanAmount           0\nLoan_Amount_Term     0\nCredit_History       0\nProperty_Area        0\nLoan_Status          0\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:39.806749Z",
     "start_time": "2023-09-28T13:37:39.476981Z"
    }
   },
   "id": "70dd9880cf2a46e7"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.635039Z",
     "start_time": "2023-09-28T13:37:39.484861Z"
    }
   },
   "id": "40e97bc8f7449257"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.635673Z",
     "start_time": "2023-09-28T13:37:41.610009Z"
    }
   },
   "id": "a1ab6d3cb41ae161"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df[label_columns] = df[label_columns].apply(labelEncoder.fit_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.716019Z",
     "start_time": "2023-09-28T13:37:41.610806Z"
    }
   },
   "id": "383e5f437b4d8283"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "     Gender  Married  Dependents  Education  Self_Employed  ApplicantIncome  \\\n0         1        0           0          0              0             5849   \n1         1        1           1          0              0             4583   \n2         1        1           0          0              1             3000   \n3         1        1           0          1              0             2583   \n4         1        0           0          0              0             6000   \n..      ...      ...         ...        ...            ...              ...   \n609       0        0           0          0              0             2900   \n610       1        1           3          0              0             4106   \n611       1        1           1          0              0             8072   \n612       1        1           2          0              0             7583   \n613       0        0           0          0              1             4583   \n\n     CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n0                  0.0       128.0             360.0             1.0   \n1               1508.0       128.0             360.0             1.0   \n2                  0.0        66.0             360.0             1.0   \n3               2358.0       120.0             360.0             1.0   \n4                  0.0       141.0             360.0             1.0   \n..                 ...         ...               ...             ...   \n609                0.0        71.0             360.0             1.0   \n610                0.0        40.0             180.0             1.0   \n611              240.0       253.0             360.0             1.0   \n612                0.0       187.0             360.0             1.0   \n613                0.0       133.0             360.0             0.0   \n\n     Property_Area  Loan_Status  \n0                2            1  \n1                0            0  \n2                2            1  \n3                2            1  \n4                2            1  \n..             ...          ...  \n609              0            1  \n610              0            1  \n611              2            1  \n612              2            1  \n613              1            0  \n\n[614 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Married</th>\n      <th>Dependents</th>\n      <th>Education</th>\n      <th>Self_Employed</th>\n      <th>ApplicantIncome</th>\n      <th>CoapplicantIncome</th>\n      <th>LoanAmount</th>\n      <th>Loan_Amount_Term</th>\n      <th>Credit_History</th>\n      <th>Property_Area</th>\n      <th>Loan_Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5849</td>\n      <td>0.0</td>\n      <td>128.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4583</td>\n      <td>1508.0</td>\n      <td>128.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3000</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2583</td>\n      <td>2358.0</td>\n      <td>120.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6000</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2900</td>\n      <td>0.0</td>\n      <td>71.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4106</td>\n      <td>0.0</td>\n      <td>40.0</td>\n      <td>180.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8072</td>\n      <td>240.0</td>\n      <td>253.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7583</td>\n      <td>0.0</td>\n      <td>187.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4583</td>\n      <td>0.0</td>\n      <td>133.0</td>\n      <td>360.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>614 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.750564Z",
     "start_time": "2023-09-28T13:37:41.651309Z"
    }
   },
   "id": "a348d48400289124"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "y = df['Loan_Status']\n",
    "x = df.drop('Loan_Status', axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.751445Z",
     "start_time": "2023-09-28T13:37:41.704557Z"
    }
   },
   "id": "567741e919e7b320"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.751795Z",
     "start_time": "2023-09-28T13:37:41.711341Z"
    }
   },
   "id": "b33f3a6f995e320d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "standartScale = StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.779286Z",
     "start_time": "2023-09-28T13:37:41.718106Z"
    }
   },
   "id": "71c25086e670e5d"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "x = standartScale.fit_transform(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.780609Z",
     "start_time": "2023-09-28T13:37:41.727431Z"
    }
   },
   "id": "63cf3d7e5560efc9"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#Voting"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:41.782105Z",
     "start_time": "2023-09-28T13:37:41.733084Z"
    }
   },
   "id": "702bea5bf6fbd911"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.397297Z",
     "start_time": "2023-09-28T13:37:41.738164Z"
    }
   },
   "id": "b6b8893ce8ad588c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "svc_model = SVC()\n",
    "tree_model = DecisionTreeClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.409824Z",
     "start_time": "2023-09-28T13:37:42.397053Z"
    }
   },
   "id": "9a53ec3c9854b8f3"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VotingClassifier in module sklearn.ensemble._voting:\n",
      "\n",
      "class VotingClassifier(sklearn.base.ClassifierMixin, _BaseVoting)\n",
      " |  VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      " |  \n",
      " |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimators : list of (str, estimator) tuples\n",
      " |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
      " |      of those original estimators that will be stored in the class attribute\n",
      " |      ``self.estimators_``. An estimator can be set to ``'drop'`` using\n",
      " |      :meth:`set_params`.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |          ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n",
      " |          support was removed in 0.24.\n",
      " |  \n",
      " |  voting : {'hard', 'soft'}, default='hard'\n",
      " |      If 'hard', uses predicted class labels for majority rule voting.\n",
      " |      Else if 'soft', predicts the class label based on the argmax of\n",
      " |      the sums of the predicted probabilities, which is recommended for\n",
      " |      an ensemble of well-calibrated classifiers.\n",
      " |  \n",
      " |  weights : array-like of shape (n_classifiers,), default=None\n",
      " |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      " |      predicted class labels (`hard` voting) or class probabilities\n",
      " |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel for ``fit``.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  flatten_transform : bool, default=True\n",
      " |      Affects shape of transform output only when voting='soft'\n",
      " |      If voting='soft' and flatten_transform=True, transform method returns\n",
      " |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
      " |      flatten_transform=False, it returns\n",
      " |      (n_classifiers, n_samples, n_classes).\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      If True, the time elapsed while fitting will be printed as it\n",
      " |      is completed.\n",
      " |  \n",
      " |      .. versionadded:: 0.23\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      " |      that are not 'drop'.\n",
      " |  \n",
      " |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      " |      Attribute to access any fitted sub-estimators by name.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  le_ : :class:`~sklearn.preprocessing.LabelEncoder`\n",
      " |      Transformer used to encode the labels during fit and decode during\n",
      " |      prediction.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`. Only defined if the\n",
      " |      underlying classifier exposes such an attribute when fit.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Only defined if the\n",
      " |      underlying estimators expose such an attribute when fit.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  VotingRegressor : Prediction voting regressor.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      " |  >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
      " |  >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
      " |  >>> clf3 = GaussianNB()\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> eclf1 = VotingClassifier(estimators=[\n",
      " |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
      " |  >>> eclf1 = eclf1.fit(X, y)\n",
      " |  >>> print(eclf1.predict(X))\n",
      " |  [1 1 1 2 2 2]\n",
      " |  >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
      " |  ...                eclf1.named_estimators_['lr'].predict(X))\n",
      " |  True\n",
      " |  >>> eclf2 = VotingClassifier(estimators=[\n",
      " |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      " |  ...         voting='soft')\n",
      " |  >>> eclf2 = eclf2.fit(X, y)\n",
      " |  >>> print(eclf2.predict(X))\n",
      " |  [1 1 1 2 2 2]\n",
      " |  \n",
      " |  To drop an estimator, :meth:`set_params` can be used to remove it. Here we\n",
      " |  dropped one of the estimators, resulting in 2 fitted estimators:\n",
      " |  \n",
      " |  >>> eclf2 = eclf2.set_params(lr='drop')\n",
      " |  >>> eclf2 = eclf2.fit(X, y)\n",
      " |  >>> len(eclf2.estimators_)\n",
      " |  2\n",
      " |  \n",
      " |  Setting `flatten_transform=True` with `voting='soft'` flattens output shape of\n",
      " |  `transform`:\n",
      " |  \n",
      " |  >>> eclf3 = VotingClassifier(estimators=[\n",
      " |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      " |  ...        voting='soft', weights=[2,1,1],\n",
      " |  ...        flatten_transform=True)\n",
      " |  >>> eclf3 = eclf3.fit(X, y)\n",
      " |  >>> print(eclf3.predict(X))\n",
      " |  [1 1 1 2 2 2]\n",
      " |  >>> print(eclf3.transform(X).shape)\n",
      " |  (6, 6)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VotingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      _BaseVoting\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the estimators.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if all underlying estimators\n",
      " |          support sample weights.\n",
      " |      \n",
      " |          .. versionadded:: 0.18\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      maj : array-like of shape (n_samples,)\n",
      " |          Predicted class labels.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      avg : array-like of shape (n_samples, n_classes)\n",
      " |          Weighted average probability for each class per sample.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.ensemble._voting.VotingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._voting.VotingClassifier\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.ensemble._voting.VotingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._voting.VotingClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Return class labels or probabilities for X for each estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      probabilities_or_labels\n",
      " |          If `voting='soft'` and `flatten_transform=True`:\n",
      " |              returns ndarray of shape (n_samples, n_classifiers * n_classes),\n",
      " |              being class probabilities calculated by each classifier.\n",
      " |          If `voting='soft' and `flatten_transform=False`:\n",
      " |              ndarray of shape (n_classifiers, n_samples, n_classes)\n",
      " |          If `voting='hard'`:\n",
      " |              ndarray of shape (n_samples, n_classifiers), being\n",
      " |              class labels predicted by each classifier.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseVoting:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Return class labels or probabilities for each estimator.\n",
      " |      \n",
      " |      Return predictions for X for each estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseVoting:\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |      \n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |      \n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Returns the parameters given in the constructor as well as the\n",
      " |      estimators contained within the `estimators` parameter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          Setting it to True gets the various estimators and the parameters\n",
      " |          of the estimators as well.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter and estimator names mapped to their values or parameter\n",
      " |          names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      " |      can directly set the parameters of the estimators contained in\n",
      " |      `estimators`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : keyword arguments\n",
      " |          Specific parameters using e.g.\n",
      " |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      " |          parameters of the estimator, the individual estimator of the\n",
      " |          estimators can also be set, or can be removed by setting them to\n",
      " |          'drop'.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  named_estimators\n",
      " |      Dictionary to access any fitted sub-estimators by name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~sklearn.utils.Bunch`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n"
     ]
    }
   ],
   "source": [
    "help(VotingClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.544091Z",
     "start_time": "2023-09-28T13:37:42.408502Z"
    }
   },
   "id": "5d20b69ec9df37b8"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators = [('log_model', log_model), ('svc_model', svc_model), ('tree_model', tree_model)]\n",
    "                          ,voting = 'hard'\n",
    "                          )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.548018Z",
     "start_time": "2023-09-28T13:37:42.428961Z"
    }
   },
   "id": "abb0dc7115a18a8d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.593895Z",
     "start_time": "2023-09-28T13:37:42.447336Z"
    }
   },
   "id": "4854bfd46439078d"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.595281Z",
     "start_time": "2023-09-28T13:37:42.457083Z"
    }
   },
   "id": "f852cdd2ad7d098"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "VotingClassifier(estimators=[('log_model', LogisticRegression()),\n                             ('svc_model', SVC()),\n                             ('tree_model', DecisionTreeClassifier())])",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;log_model&#x27;, LogisticRegression()),\n                             (&#x27;svc_model&#x27;, SVC()),\n                             (&#x27;tree_model&#x27;, DecisionTreeClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;log_model&#x27;, LogisticRegression()),\n                             (&#x27;svc_model&#x27;, SVC()),\n                             (&#x27;tree_model&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>log_model</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc_model</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tree_model</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.604153Z",
     "start_time": "2023-09-28T13:37:42.471014Z"
    }
   },
   "id": "2ea491c4d77d765f"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "voting_pred = voting.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.650833Z",
     "start_time": "2023-09-28T13:37:42.533961Z"
    }
   },
   "id": "a6478caa3f280e46"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.786356Z",
     "start_time": "2023-09-28T13:37:42.547725Z"
    }
   },
   "id": "850b0c7ea17a6733"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8048780487804879"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, voting_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.838294Z",
     "start_time": "2023-09-28T13:37:42.553151Z"
    }
   },
   "id": "93db4ed4bf152436"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "#SOFT VOTING"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.839056Z",
     "start_time": "2023-09-28T13:37:42.567168Z"
    }
   },
   "id": "533fa479956c68bd"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "svc_model = SVC(probability = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.839771Z",
     "start_time": "2023-09-28T13:37:42.574129Z"
    }
   },
   "id": "71540397fc1876cc"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "voting_soft = VotingClassifier(estimators = [('log_model', log_model), ('svc_model', svc_model), ('tree_model', tree_model)]\n",
    "                          ,voting = 'soft'\n",
    "                          )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.840498Z",
     "start_time": "2023-09-28T13:37:42.582948Z"
    }
   },
   "id": "406725eaab3302bf"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "VotingClassifier(estimators=[('log_model', LogisticRegression()),\n                             ('svc_model', SVC(probability=True)),\n                             ('tree_model', DecisionTreeClassifier())],\n                 voting='soft')",
      "text/html": "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;log_model&#x27;, LogisticRegression()),\n                             (&#x27;svc_model&#x27;, SVC(probability=True)),\n                             (&#x27;tree_model&#x27;, DecisionTreeClassifier())],\n                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;log_model&#x27;, LogisticRegression()),\n                             (&#x27;svc_model&#x27;, SVC(probability=True)),\n                             (&#x27;tree_model&#x27;, DecisionTreeClassifier())],\n                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>log_model</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc_model</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>tree_model</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_soft.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.926097Z",
     "start_time": "2023-09-28T13:37:42.602012Z"
    }
   },
   "id": "aba3329116aad970"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "voting_soft_pred = voting_soft.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.926905Z",
     "start_time": "2023-09-28T13:37:42.695813Z"
    }
   },
   "id": "b21eef7ffc94c703"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7804878048780488"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, voting_soft_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.953304Z",
     "start_time": "2023-09-28T13:37:42.706093Z"
    }
   },
   "id": "698d82e1b881baee"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.953883Z",
     "start_time": "2023-09-28T13:37:42.716036Z"
    }
   },
   "id": "27014c9f7cd97e3e"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BaggingClassifier in module sklearn.ensemble._bagging:\n",
      "\n",
      "class BaggingClassifier(sklearn.base.ClassifierMixin, BaseBagging)\n",
      " |  BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')\n",
      " |  \n",
      " |  A Bagging classifier.\n",
      " |  \n",
      " |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      " |  classifiers each on random subsets of the original dataset and then\n",
      " |  aggregate their individual predictions (either by voting or by averaging)\n",
      " |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      " |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      " |  tree), by introducing randomization into its construction procedure and\n",
      " |  then making an ensemble out of it.\n",
      " |  \n",
      " |  This algorithm encompasses several works from the literature. When random\n",
      " |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      " |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      " |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      " |  of the dataset are drawn as random subsets of the features, then the method\n",
      " |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      " |  on subsets of both samples and features, then the method is known as\n",
      " |  Random Patches [4]_.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <bagging>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.15\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : object, default=None\n",
      " |      The base estimator to fit on random subsets of the dataset.\n",
      " |      If None, then the base estimator is a\n",
      " |      :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator` was renamed to `estimator`.\n",
      " |  \n",
      " |  n_estimators : int, default=10\n",
      " |      The number of base estimators in the ensemble.\n",
      " |  \n",
      " |  max_samples : int or float, default=1.0\n",
      " |      The number of samples to draw from X to train each base estimator (with\n",
      " |      replacement by default, see `bootstrap` for more details).\n",
      " |  \n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      " |  \n",
      " |  max_features : int or float, default=1.0\n",
      " |      The number of features to draw from X to train each base estimator (\n",
      " |      without replacement by default, see `bootstrap_features` for more\n",
      " |      details).\n",
      " |  \n",
      " |      - If int, then draw `max_features` features.\n",
      " |      - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether samples are drawn with replacement. If False, sampling\n",
      " |      without replacement is performed.\n",
      " |  \n",
      " |  bootstrap_features : bool, default=False\n",
      " |      Whether features are drawn with replacement.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization error. Only available if bootstrap=True.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit\n",
      " |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* constructor parameter.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      " |      :meth:`predict`. ``None`` means 1 unless in a\n",
      " |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      " |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random resampling of the original dataset\n",
      " |      (sample wise and feature wise).\n",
      " |      If the base estimator accepts a `random_state` attribute, a different\n",
      " |      seed is generated for each instance in the ensemble.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  base_estimator : object, default=\"deprecated\"\n",
      " |      Use `estimator` instead.\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |          `base_estimator` is deprecated and will be removed in 1.4.\n",
      " |          Use `estimator` instead.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |      .. versionadded:: 1.2\n",
      " |         `base_estimator_` was renamed to `estimator_`.\n",
      " |  \n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |      .. deprecated:: 1.2\n",
      " |          `base_estimator_` is deprecated and will be removed in 1.4.\n",
      " |          Use `estimator_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  estimators_ : list of estimators\n",
      " |      The collection of fitted base estimators.\n",
      " |  \n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator. Each subset is defined by an array of the indices selected.\n",
      " |  \n",
      " |  estimators_features_ : list of arrays\n",
      " |      The subset of drawn features for each base estimator.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  BaggingRegressor : A Bagging regressor.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      " |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      " |  \n",
      " |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      " |         1996.\n",
      " |  \n",
      " |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      " |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      " |         1998.\n",
      " |  \n",
      " |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      " |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> from sklearn.ensemble import BaggingClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=100, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = BaggingClassifier(estimator=SVC(),\n",
      " |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BaggingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseBagging\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Average of the decision functions of the base classifiers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples. The columns correspond\n",
      " |          to the classes in sorted order, as they appear in the attribute\n",
      " |          ``classes_``. Regression and binary classification are special\n",
      " |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the class with\n",
      " |      the highest mean predicted probability. If base estimators do not\n",
      " |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the base\n",
      " |      estimators in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the mean predicted class probabilities of the base estimators in the\n",
      " |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      " |      method, then it resorts to voting and the predicted class probabilities\n",
      " |      of an input sample represents the proportion of estimators predicting\n",
      " |      each class.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.ensemble._bagging.BaggingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._bagging.BaggingClassifier\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.ensemble._bagging.BaggingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._bagging.BaggingClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseBagging:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a Bagging ensemble of estimators from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if the base estimator supports\n",
      " |          sample weighting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseBagging:\n",
      " |  \n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |      \n",
      " |      Returns a dynamically generated list of indices identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |      \n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  base_estimator_\n",
      " |      Estimator used to grow the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n"
     ]
    }
   ],
   "source": [
    "help(BaggingClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.976065Z",
     "start_time": "2023-09-28T13:37:42.726349Z"
    }
   },
   "id": "644a0a8ab2f9e798"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(base_estimator = svc_model, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:42.976687Z",
     "start_time": "2023-09-28T13:37:42.738396Z"
    }
   },
   "id": "85f4f4d68476fa80"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismile/PycharmProjects/selenium/venv/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "BaggingClassifier(base_estimator=SVC(probability=True), random_state=42)",
      "text/html": "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(base_estimator=SVC(probability=True), random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(base_estimator=SVC(probability=True), random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.327299Z",
     "start_time": "2023-09-28T13:37:42.749868Z"
    }
   },
   "id": "5aa01e1f66c6d4bc"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "bagging_pred = bagging.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.330693Z",
     "start_time": "2023-09-28T13:37:43.057802Z"
    }
   },
   "id": "e4e15cb37dac67f8"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7804878048780488"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, bagging_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.332615Z",
     "start_time": "2023-09-28T13:37:43.091902Z"
    }
   },
   "id": "17a6a987b27bb9a2"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.333449Z",
     "start_time": "2023-09-28T13:37:43.113007Z"
    }
   },
   "id": "e722c1afb044317f"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "gradientBoosting = GradientBoostingClassifier(learning_rate = 0.01, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.339863Z",
     "start_time": "2023-09-28T13:37:43.118638Z"
    }
   },
   "id": "a25fe1ed6511c291"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "GradientBoostingClassifier(learning_rate=0.01, random_state=42)",
      "text/html": "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.01, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, random_state=42)</pre></div></div></div></div></div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientBoosting.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.445581Z",
     "start_time": "2023-09-28T13:37:43.128116Z"
    }
   },
   "id": "571f6f59d2d7fc40"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "gradientBoosting_pred = gradientBoosting.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.621711Z",
     "start_time": "2023-09-28T13:37:43.331219Z"
    }
   },
   "id": "5861feaa8739aa83"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7804878048780488"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, gradientBoosting_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.656992Z",
     "start_time": "2023-09-28T13:37:43.340874Z"
    }
   },
   "id": "477aa5b77af09796"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.659467Z",
     "start_time": "2023-09-28T13:37:43.351059Z"
    }
   },
   "id": "ee85520bd8a88a25"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, XGBClassifierMixIn, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', **kwargs: Any) -> None\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  See :doc:`/python/sklearn_estimator` for more information.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : Optional[int]\n",
      " |          Number of boosting rounds.\n",
      " |  \n",
      " |      max_depth :  Optional[int]\n",
      " |          Maximum tree depth for base learners.\n",
      " |      max_leaves :\n",
      " |          Maximum number of leaves; 0 indicates no limit.\n",
      " |      max_bin :\n",
      " |          If using histogram-based algorithm, maximum number of bins per feature\n",
      " |      grow_policy :\n",
      " |          Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
      " |          depth-wise. 1: favor splitting at nodes with highest loss change.\n",
      " |      learning_rate : Optional[float]\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : Optional[int]\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: Optional[str]\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: Optional[str]\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      " |          default, XGBoost will choose the most conservative option available.  It's\n",
      " |          recommended to study this option from the parameters document :doc:`tree method\n",
      " |          </treemethod>`\n",
      " |      n_jobs : Optional[int]\n",
      " |          Number of parallel threads used to run xgboost.  When used with other\n",
      " |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
      " |          parallelize and balance the threads.  Creating thread contention will\n",
      " |          significantly slow down both algorithms.\n",
      " |      gamma : Optional[float]\n",
      " |          (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
      " |          leaf node of the tree.\n",
      " |      min_child_weight : Optional[float]\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : Optional[float]\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : Optional[float]\n",
      " |          Subsample ratio of the training instance.\n",
      " |      sampling_method :\n",
      " |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
      " |            - ``uniform``: select random training instances uniformly.\n",
      " |            - ``gradient_based`` select random training instances with higher probability\n",
      " |              when the gradient and hessian are larger. (cf. CatBoost)\n",
      " |      colsample_bytree : Optional[float]\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : Optional[float]\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : Optional[float]\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : Optional[float]\n",
      " |          L1 regularization term on weights (xgb's alpha).\n",
      " |      reg_lambda : Optional[float]\n",
      " |          L2 regularization term on weights (xgb's lambda).\n",
      " |      scale_pos_weight : Optional[float]\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score : Optional[float]\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: Optional[int]\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      " |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      " |          for more information.\n",
      " |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      " |          3, 4]]``, where each inner list is a group of indices of features that are\n",
      " |          allowed to interact with each other.  See :doc:`tutorial\n",
      " |          </tutorials/feature_interaction_constraint>` for more information\n",
      " |      importance_type: Optional[str]\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |  \n",
      " |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      " |            \"total_cover\".\n",
      " |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      " |            without bias.\n",
      " |  \n",
      " |      device : Optional[str]\n",
      " |  \n",
      " |          .. versionadded:: 2.0.0\n",
      " |  \n",
      " |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
      " |  \n",
      " |      validate_parameters : Optional[bool]\n",
      " |  \n",
      " |          Give warnings for unknown parameter.\n",
      " |  \n",
      " |      enable_categorical : bool\n",
      " |  \n",
      " |          .. versionadded:: 1.5.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
      " |          should be used to specify categorical data type.  Also, JSON/UBJSON\n",
      " |          serialization format is required.\n",
      " |  \n",
      " |      feature_types : Optional[FeatureTypes]\n",
      " |  \n",
      " |          .. versionadded:: 1.7.0\n",
      " |  \n",
      " |          Used for specifying feature types without constructing a dataframe. See\n",
      " |          :py:class:`DMatrix` for details.\n",
      " |  \n",
      " |      max_cat_to_onehot : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      " |          for categorical data.  When number of categories is lesser than the threshold\n",
      " |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      " |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
      " |          categorical feature support. See :doc:`Categorical Data\n",
      " |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      " |  \n",
      " |      max_cat_threshold : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.7.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          Maximum number of categories considered for each split. Used only by\n",
      " |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
      " |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
      " |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      " |  \n",
      " |      multi_strategy : Optional[str]\n",
      " |  \n",
      " |          .. versionadded:: 2.0.0\n",
      " |  \n",
      " |          .. note:: This parameter is working-in-progress.\n",
      " |  \n",
      " |          The strategy used for training multi-target models, including multi-target\n",
      " |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
      " |          more information.\n",
      " |  \n",
      " |          - ``one_output_per_tree``: One model for each target.\n",
      " |          - ``multi_output_tree``:  Use multi-target trees.\n",
      " |  \n",
      " |      eval_metric : Optional[Union[str, List[str], Callable]]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Metric used for monitoring the training result and early stopping.  It can be a\n",
      " |          string or list of strings as names of predefined metric in XGBoost (See\n",
      " |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
      " |          user defined metric that looks like `sklearn.metrics`.\n",
      " |  \n",
      " |          If custom objective is also provided, then custom metric should implement the\n",
      " |          corresponding reverse link function.\n",
      " |  \n",
      " |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      " |          object is provided, it's assumed to be a cost function and by default XGBoost will\n",
      " |          minimize the result during early stopping.\n",
      " |  \n",
      " |          For advanced usage on Early stopping like directly choosing to maximize instead of\n",
      " |          minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      " |  \n",
      " |          See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
      " |          for more.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |               This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old\n",
      " |               one receives un-transformed prediction regardless of whether custom\n",
      " |               objective is being used.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              from sklearn.datasets import load_diabetes\n",
      " |              from sklearn.metrics import mean_absolute_error\n",
      " |              X, y = load_diabetes(return_X_y=True)\n",
      " |              reg = xgb.XGBRegressor(\n",
      " |                  tree_method=\"hist\",\n",
      " |                  eval_metric=mean_absolute_error,\n",
      " |              )\n",
      " |              reg.fit(X, y, eval_set=[(X, y)])\n",
      " |  \n",
      " |      early_stopping_rounds : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          - Activates early stopping. Validation metric needs to improve at least once in\n",
      " |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
      " |            least one item in **eval_set** in :py:meth:`fit`.\n",
      " |  \n",
      " |          - If early stopping occurs, the model will have two additional attributes:\n",
      " |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
      " |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
      " |            number of trees during inference. If users want to access the full model\n",
      " |            (including trees built after early stopping), they can specify the\n",
      " |            `iteration_range` in these inference methods. In addition, other utilities\n",
      " |            like model plotting can also use the entire model.\n",
      " |  \n",
      " |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
      " |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
      " |  \n",
      " |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
      " |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
      " |            metric will be used for early stopping.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |              This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
      " |  \n",
      " |      callbacks : Optional[List[TrainingCallback]]\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using\n",
      " |          :ref:`Callback API <callback_api>`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             States in callback are not preserved during training, which means callback\n",
      " |             objects can not be reused for multiple training sessions without\n",
      " |             reinitialization or deepcopy.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              for params in parameters_grid:\n",
      " |                  # be sure to (re)initialize the callbacks before each run\n",
      " |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      " |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
      " |                  reg.fit(X, y)\n",
      " |  \n",
      " |      kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      " |          can be found :doc:`here </parameter>`.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      XGBClassifierMixIn\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', **kwargs: Any) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, eval_metric: Union[str, Sequence[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None) -> 'XGBClassifier'\n",
      " |      Fit gradient boosting classifier.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Feature matrix. See :ref:`py-data` for a list of supported types.\n",
      " |      \n",
      " |          When the ``tree_method`` is set to ``hist``, internally, the\n",
      " |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
      " |          for conserving memory. However, this has performance implications when the\n",
      " |          device of input data is not matched with algorithm. For instance, if the\n",
      " |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
      " |          data is first processed on CPU then transferred to GPU.\n",
      " |      y :\n",
      " |          Labels\n",
      " |      sample_weight :\n",
      " |          instance weights\n",
      " |      base_margin :\n",
      " |          global bias for each instance.\n",
      " |      eval_set :\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      \n",
      " |      eval_metric : str, list of str, or callable, optional\n",
      " |      \n",
      " |          .. deprecated:: 1.6.0\n",
      " |      \n",
      " |          Use `eval_metric` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |      \n",
      " |          .. deprecated:: 1.6.0\n",
      " |      \n",
      " |          Use `early_stopping_rounds` in :py:meth:`__init__` or :py:meth:`set_params`\n",
      " |          instead.\n",
      " |      verbose :\n",
      " |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
      " |          measured on the validation set is printed to stdout at each boosting stage.\n",
      " |          If `verbose` is an integer, the evaluation metric is printed at each\n",
      " |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
      " |          by using `early_stopping_rounds` is also printed.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set :\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set :\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights :\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.\n",
      " |      \n",
      " |      callbacks :\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `callbacks` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |  \n",
      " |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the various\n",
      " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      " |      feature_names) will not be loaded when using binary format.  To save those\n",
      " |      attributes, use JSON/UBJ instead.  See :doc:`Model IO </tutorials/saving_model>`\n",
      " |      for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.load_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.load_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  predict(self, X: Any, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> Any\n",
      " |      Predict with `X`.  If the model is trained with early stopping, then\n",
      " |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
      " |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
      " |      devices between the data and the estimator don't match.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Data to predict with.\n",
      " |      output_margin :\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin :\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      " |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
      " |          are used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction\n",
      " |  \n",
      " |  predict_proba(self, X: Any, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Predict the probability of each `X` example being of a given class. If the\n",
      " |      model is trained with early stopping, then :py:attr:`best_iteration` is used\n",
      " |      automatically. The estimator uses `inplace_predict` by default and falls back to\n",
      " |      using :py:class:`DMatrix` if devices between the data and the estimator don't\n",
      " |      match.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Feature matrix. See :ref:`py-data` for a list of supported types.\n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin :\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      " |          probability of each data example being of a given class.\n",
      " |  \n",
      " |  set_fit_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', callbacks: Union[bool, NoneType, str] = '$UNCHANGED$', early_stopping_rounds: Union[bool, NoneType, str] = '$UNCHANGED$', eval_metric: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
      " |      \n",
      " |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
      " |      \n",
      " |      callbacks : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``callbacks`` parameter in ``fit``.\n",
      " |      \n",
      " |      early_stopping_rounds : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``early_stopping_rounds`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_metric : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_metric`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
      " |      \n",
      " |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
      " |      \n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
      " |      \n",
      " |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
      " |      \n",
      " |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_predict_proba_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier\n",
      " |      Request metadata passed to the ``predict_proba`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``base_margin`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``iteration_range`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``validate_features`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_predict_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier\n",
      " |      Request metadata passed to the ``predict`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
      " |      \n",
      " |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
      " |      \n",
      " |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
      " |      \n",
      " |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: xgboost.sklearn.XGBClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  apply(self, X: Any, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample. If the model is trained\n",
      " |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      iteration_range :\n",
      " |          See :py:meth:`predict`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
      " |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
      " |      function.\n",
      " |      \n",
      " |      The returned evaluation result is a dictionary:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result\n",
      " |  \n",
      " |  get_booster(self) -> xgboost.core.Booster\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self) -> int\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self) -> Dict[str, Any]\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      " |      (such as feature_names) will not be saved when using binary format.  To save\n",
      " |      those attributes, use JSON/UBJ instead. See :doc:`Model IO\n",
      " |      </tutorials/saving_model>` for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.save_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.save_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params: Any) -> 'XGBModel'\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
      " |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
      " |  \n",
      " |  best_score\n",
      " |      The best score obtained by early stopping.\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property, return depends on `importance_type`\n",
      " |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
      " |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
      " |      based on the importance type. For instance, if the importance type is\n",
      " |      \"total_gain\", then the score is sum of loss change for each split from all\n",
      " |      trees.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      " |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      " |  \n",
      " |  feature_names_in_\n",
      " |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
      " |      feature names that are all strings.\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types,\n",
      " |          such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :py:meth:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"
     ]
    }
   ],
   "source": [
    "help(XGBClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.680024Z",
     "start_time": "2023-09-28T13:37:43.438820Z"
    }
   },
   "id": "efbdca3bd730708a"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier(max_depth = 100, learning_rate = 0.01, random_state = 42, n_estimators = 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:43.680972Z",
     "start_time": "2023-09-28T13:37:43.454086Z"
    }
   },
   "id": "8416f936f10169e4"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismile/PycharmProjects/selenium/venv/lib/python3.11/site-packages/xgboost/data.py:520: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=100, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)",
      "text/html": "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=100, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=100, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:44.705326Z",
     "start_time": "2023-09-28T13:37:43.461538Z"
    }
   },
   "id": "21bbb4fa2e3b46ac"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "xgboost_pred = xgboost.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:44.709324Z",
     "start_time": "2023-09-28T13:37:44.703154Z"
    }
   },
   "id": "978b0ecfb8778f02"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7804878048780488"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, xgboost_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:37:44.729968Z",
     "start_time": "2023-09-28T13:37:44.711710Z"
    }
   },
   "id": "106c2a47c7c382c4"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:41:13.449708Z",
     "start_time": "2023-09-28T13:41:13.114161Z"
    }
   },
   "id": "4e84346d23b3a1ff"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CatBoostClassifier in module catboost.core:\n",
      "\n",
      "class CatBoostClassifier(CatBoost)\n",
      " |  CatBoostClassifier(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, random_score_type=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for CatBoost classification.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  iterations : int, [default=500]\n",
      " |      Max count of trees.\n",
      " |      range: [1,+inf)\n",
      " |  learning_rate : float, [default value is selected automatically for binary classification with other parameters set to default. In all other cases default is 0.03]\n",
      " |      Step size shrinkage used in update to prevents overfitting.\n",
      " |      range: (0,1]\n",
      " |  depth : int, [default=6]\n",
      " |      Depth of a tree. All trees are the same depth.\n",
      " |      range: [1,16]\n",
      " |  l2_leaf_reg : float, [default=3.0]\n",
      " |      Coefficient at the L2 regularization term of the cost function.\n",
      " |      range: [0,+inf)\n",
      " |  model_size_reg : float, [default=None]\n",
      " |      Model size regularization coefficient.\n",
      " |      range: [0,+inf)\n",
      " |  rsm : float, [default=None]\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |      range: (0,1]\n",
      " |  loss_function : string or object, [default='Logloss']\n",
      " |      The metric to use in training and also selector of the machine learning\n",
      " |      problem to solve. If string, then the name of a supported metric,\n",
      " |      optionally suffixed with parameter description.\n",
      " |      If object, it shall provide methods 'calc_ders_range' or 'calc_ders_multi'.\n",
      " |  border_count : int, [default = 254 for training on CPU or 128 for training on GPU]\n",
      " |      The number of partitions in numeric features binarization. Used in the preliminary calculation.\n",
      " |      range: [1,65535] on CPU, [1,255] on GPU\n",
      " |  feature_border_type : string, [default='GreedyLogSum']\n",
      " |      The binarization mode in numeric features binarization. Used in the preliminary calculation.\n",
      " |      Possible values:\n",
      " |          - 'Median'\n",
      " |          - 'Uniform'\n",
      " |          - 'UniformAndQuantiles'\n",
      " |          - 'GreedyLogSum'\n",
      " |          - 'MaxLogSum'\n",
      " |          - 'MinEntropy'\n",
      " |  per_float_feature_quantization : list of strings, [default=None]\n",
      " |      List of float binarization descriptions.\n",
      " |      Format : described in documentation on catboost.ai\n",
      " |      Example 1: ['0:1024'] means that feature 0 will have 1024 borders.\n",
      " |      Example 2: ['0:border_count=1024', '1:border_count=1024', ...] means that two first features have 1024 borders.\n",
      " |      Example 3: ['0:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum',\n",
      " |                  '1:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum'] - defines more quantization properties for first two features.\n",
      " |  input_borders : string or pathlib.Path, [default=None]\n",
      " |      input file with borders used in numeric features binarization.\n",
      " |  output_borders : string, [default=None]\n",
      " |      output file for borders that were used in numeric features binarization.\n",
      " |  fold_permutation_block : int, [default=1]\n",
      " |      To accelerate the learning.\n",
      " |      The recommended value is within [1, 256]. On small samples, must be set to 1.\n",
      " |      range: [1,+inf)\n",
      " |  od_pval : float, [default=None]\n",
      " |      Use overfitting detector to stop training when reaching a specified threshold.\n",
      " |      Can be used only with eval_set.\n",
      " |      range: [0,1]\n",
      " |  od_wait : int, [default=None]\n",
      " |      Number of iterations which overfitting detector will wait after new best error.\n",
      " |  od_type : string, [default=None]\n",
      " |      Type of overfitting detector which will be used in program.\n",
      " |      Posible values:\n",
      " |          - 'IncToDec'\n",
      " |          - 'Iter'\n",
      " |      For 'Iter' type od_pval must not be set.\n",
      " |      If None, then od_type=IncToDec.\n",
      " |  nan_mode : string, [default=None]\n",
      " |      Way to process missing values for numeric features.\n",
      " |      Possible values:\n",
      " |          - 'Forbidden' - raises an exception if there is a missing value for a numeric feature in a dataset.\n",
      " |          - 'Min' - each missing value will be processed as the minimum numerical value.\n",
      " |          - 'Max' - each missing value will be processed as the maximum numerical value.\n",
      " |      If None, then nan_mode=Min.\n",
      " |  counter_calc_method : string, [default=None]\n",
      " |      The method used to calculate counters for dataset with Counter type.\n",
      " |      Possible values:\n",
      " |          - 'PrefixTest' - only objects up to current in the test dataset are considered\n",
      " |          - 'FullTest' - all objects are considered in the test dataset\n",
      " |          - 'SkipTest' - Objects from test dataset are not considered\n",
      " |          - 'Full' - all objects are considered for both learn and test dataset\n",
      " |      If None, then counter_calc_method=PrefixTest.\n",
      " |  leaf_estimation_iterations : int, [default=None]\n",
      " |      The number of steps in the gradient when calculating the values in the leaves.\n",
      " |      If None, then leaf_estimation_iterations=1.\n",
      " |      range: [1,+inf)\n",
      " |  leaf_estimation_method : string, [default=None]\n",
      " |      The method used to calculate the values in the leaves.\n",
      " |      Possible values:\n",
      " |          - 'Newton'\n",
      " |          - 'Gradient'\n",
      " |  thread_count : int, [default=None]\n",
      " |      Number of parallel threads used to run CatBoost.\n",
      " |      If None or -1, then the number of threads is set to the number of CPU cores.\n",
      " |      range: [1,+inf)\n",
      " |  random_seed : int, [default=None]\n",
      " |      Random number seed.\n",
      " |      If None, 0 is used.\n",
      " |      range: [0,+inf)\n",
      " |  use_best_model : bool, [default=None]\n",
      " |      To limit the number of trees in predict() using information about the optimal value of the error function.\n",
      " |      Can be used only with eval_set.\n",
      " |  best_model_min_trees : int, [default=None]\n",
      " |      The minimal number of trees the best model should have.\n",
      " |  verbose: bool\n",
      " |      When set to True, logging_level is set to 'Verbose'.\n",
      " |      When set to False, logging_level is set to 'Silent'.\n",
      " |  silent: bool, synonym for verbose\n",
      " |  logging_level : string, [default='Verbose']\n",
      " |      Possible values:\n",
      " |          - 'Silent'\n",
      " |          - 'Verbose'\n",
      " |          - 'Info'\n",
      " |          - 'Debug'\n",
      " |  metric_period : int, [default=1]\n",
      " |      The frequency of iterations to print the information to stdout. The value should be a positive integer.\n",
      " |  simple_ctr: list of strings, [default=None]\n",
      " |      Binarization settings for categorical features.\n",
      " |          Format : see documentation\n",
      " |          Example: ['Borders:CtrBorderCount=5:Prior=0:Prior=0.5', 'BinarizedTargetMeanValue:TargetBorderCount=10:TargetBorderType=MinEntropy', ...]\n",
      " |          CTR types:\n",
      " |              CPU and GPU\n",
      " |              - 'Borders'\n",
      " |              - 'Buckets'\n",
      " |              CPU only\n",
      " |              - 'BinarizedTargetMeanValue'\n",
      " |              - 'Counter'\n",
      " |              GPU only\n",
      " |              - 'FloatTargetMeanValue'\n",
      " |              - 'FeatureFreq'\n",
      " |          Number_of_borders, binarization type, target borders and binarizations, priors are optional parametrs\n",
      " |  combinations_ctr: list of strings, [default=None]\n",
      " |  per_feature_ctr: list of strings, [default=None]\n",
      " |  ctr_target_border_count: int, [default=None]\n",
      " |      Maximum number of borders used in target binarization for categorical features that need it.\n",
      " |      If TargetBorderCount is specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option it\n",
      " |      overrides this value.\n",
      " |      range: [1, 255]\n",
      " |  ctr_leaf_count_limit : int, [default=None]\n",
      " |      The maximum number of leaves with categorical features.\n",
      " |      If the number of leaves exceeds the specified limit, some leaves are discarded.\n",
      " |      The leaves to be discarded are selected as follows:\n",
      " |          - The leaves are sorted by the frequency of the values.\n",
      " |          - The top N leaves are selected, where N is the value specified in the parameter.\n",
      " |          - All leaves starting from N+1 are discarded.\n",
      " |      This option reduces the resulting model size\n",
      " |      and the amount of memory required for training.\n",
      " |      Note that the resulting quality of the model can be affected.\n",
      " |      range: [1,+inf) (for zero limit use ignored_features)\n",
      " |  store_all_simple_ctr : bool, [default=None]\n",
      " |      Ignore categorical features, which are not used in feature combinations,\n",
      " |      when choosing candidates for exclusion.\n",
      " |      Use this parameter with ctr_leaf_count_limit only.\n",
      " |  max_ctr_complexity : int, [default=4]\n",
      " |      The maximum number of Categ features that can be combined.\n",
      " |      range: [0,+inf)\n",
      " |  has_time : bool, [default=False]\n",
      " |      To use the order in which objects are represented in the input data\n",
      " |      (do not perform a random permutation of the dataset at the preprocessing stage).\n",
      " |  allow_const_label : bool, [default=False]\n",
      " |      To allow the constant label value in dataset.\n",
      " |  target_border: float, [default=None]\n",
      " |      Border for target binarization.\n",
      " |  classes_count : int, [default=None]\n",
      " |      The upper limit for the numeric class label.\n",
      " |      Defines the number of classes for multiclassification.\n",
      " |      Only non-negative integers can be specified.\n",
      " |      The given integer should be greater than any of the target values.\n",
      " |      If this parameter is specified the labels for all classes in the input dataset\n",
      " |      should be smaller than the given value.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  class_weights : list or dict, [default=None]\n",
      " |      Classes weights. The values are used as multipliers for the object weights.\n",
      " |      If None, all classes are supposed to have weight one.\n",
      " |      If list - class weights in order of class_names or sequential classes if class_names is undefined\n",
      " |      If dict - dict of class_name -> class_weight.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  auto_class_weights : string [default=None]\n",
      " |      Enables automatic class weights calculation. Possible values:\n",
      " |          - Balanced  # weight = maxSummaryClassWeight / summaryClassWeight, statistics determined from train pool\n",
      " |          - SqrtBalanced  # weight = sqrt(maxSummaryClassWeight / summaryClassWeight)\n",
      " |  class_names: list of strings, [default=None]\n",
      " |      Class names. Allows to redefine the default values for class labels (integer numbers).\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  one_hot_max_size : int, [default=None]\n",
      " |      Convert the feature to float\n",
      " |      if the number of different values that it takes exceeds the specified value.\n",
      " |      Ctrs are not calculated for such features.\n",
      " |  random_strength : float, [default=1]\n",
      " |      Score standard deviation multiplier.\n",
      " |  random_score_type : string [default=None]\n",
      " |      Type of random noise added to scores.\n",
      " |      Possible values:\n",
      " |          - 'Gumbel' - Gumbel-distributed\n",
      " |          - 'NormalWithModelSizeDecrease' - Normally-distributed with deviation decreasing with model iteration count\n",
      " |      If None than 'NormalWithModelSizeDecrease' will be used by default.\n",
      " |  name : string, [default='experiment']\n",
      " |      The name that should be displayed in the visualization tools.\n",
      " |  ignored_features : list, [default=None]\n",
      " |      Indices or names of features that should be excluded when training.\n",
      " |  train_dir : string or pathlib.Path, [default=None]\n",
      " |      The directory in which you want to record generated in the process of learning files.\n",
      " |  custom_metric : string or list of strings, [default=None]\n",
      " |      To use your own metric function.\n",
      " |  custom_loss: alias to custom_metric\n",
      " |  eval_metric : string or object, [default=None]\n",
      " |      To optimize your custom metric in loss.\n",
      " |  bagging_temperature : float, [default=None]\n",
      " |      Controls intensity of Bayesian bagging. The higher the temperature the more aggressive bagging is.\n",
      " |      Typical values are in range [0, 1] (0 - no bagging, 1 - default).\n",
      " |  save_snapshot : bool, [default=None]\n",
      " |      Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |  snapshot_file : string or pathlib.Path, [default=None]\n",
      " |      Learn progress snapshot file path, if None will use default filename\n",
      " |  snapshot_interval: int, [default=600]\n",
      " |      Interval between saving snapshots (seconds)\n",
      " |  fold_len_multiplier : float, [default=None]\n",
      " |      Fold length multiplier. Should be greater than 1\n",
      " |  used_ram_limit : string or number, [default=None]\n",
      " |      Set a limit on memory consumption (value like '1.2gb' or 1.2e9).\n",
      " |      WARNING: Currently this option affects CTR memory usage only.\n",
      " |  gpu_ram_part : float, [default=0.95]\n",
      " |      Fraction of the GPU RAM to use for training, a value from (0, 1].\n",
      " |  pinned_memory_size: int [default=None]\n",
      " |      Size of additional CPU pinned memory used for GPU learning,\n",
      " |      usually is estimated automatically, thus usually should not be set.\n",
      " |  allow_writing_files : bool, [default=True]\n",
      " |      If this flag is set to False, no files with different diagnostic info will be created during training.\n",
      " |      With this flag no snapshotting can be done. Plus visualisation will not\n",
      " |      work, because visualisation uses files that are created and updated during training.\n",
      " |  final_ctr_computation_mode : string, [default='Default']\n",
      " |      Possible values:\n",
      " |          - 'Default' - Compute final ctrs for all pools.\n",
      " |          - 'Skip' - Skip final ctr computation. WARNING: model without ctrs can't be applied.\n",
      " |  approx_on_full_history : bool, [default=False]\n",
      " |      If this flag is set to True, each approximated value is calculated using all the preceeding rows in the fold (slower, more accurate).\n",
      " |      If this flag is set to False, each approximated value is calculated using only the beginning 1/fold_len_multiplier fraction of the fold (faster, slightly less accurate).\n",
      " |  boosting_type : string, default value depends on object count and feature count in train dataset and on learning mode.\n",
      " |      Boosting scheme.\n",
      " |      Possible values:\n",
      " |          - 'Ordered' - Gives better quality, but may slow down the training.\n",
      " |          - 'Plain' - The classic gradient boosting scheme. May result in quality degradation, but does not slow down the training.\n",
      " |  task_type : string, [default=None]\n",
      " |      The calcer type used to train the model.\n",
      " |      Possible values:\n",
      " |          - 'CPU'\n",
      " |          - 'GPU'\n",
      " |  device_config : string, [default=None], deprecated, use devices instead\n",
      " |  devices : list or string, [default=None], GPU devices to use.\n",
      " |      String format is: '0' for 1 device or '0:1:3' for multiple devices or '0-3' for range of devices.\n",
      " |      List format is : [0] for 1 device or [0,1,3] for multiple devices.\n",
      " |  \n",
      " |  bootstrap_type : string, Bayesian, Bernoulli, Poisson, MVS.\n",
      " |      Default bootstrap is Bayesian for GPU and MVS for CPU.\n",
      " |      Poisson bootstrap is supported only on GPU.\n",
      " |      MVS bootstrap is supported only on GPU.\n",
      " |  \n",
      " |  subsample : float, [default=None]\n",
      " |      Sample rate for bagging. This parameter can be used Poisson or Bernoully bootstrap types.\n",
      " |  \n",
      " |  mvs_reg : float, [default is set automatically at each iteration based on gradient distribution]\n",
      " |      Regularization parameter for MVS sampling algorithm\n",
      " |  \n",
      " |  monotone_constraints : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Monotone constraints for features.\n",
      " |  \n",
      " |  feature_weights : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Coefficient to multiply split gain with specific feature use. Should be non-negative.\n",
      " |  \n",
      " |  penalties_coefficient : float, [default=1]\n",
      " |      Common coefficient for all penalties. Should be non-negative.\n",
      " |  \n",
      " |  first_feature_use_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties to first use of specific feature in model. Should be non-negative.\n",
      " |  \n",
      " |  per_object_feature_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties for first use of feature for each object. Should be non-negative.\n",
      " |  \n",
      " |  sampling_frequency : string, [default=PerTree]\n",
      " |      Frequency to sample weights and objects when building trees.\n",
      " |      Possible values:\n",
      " |          - 'PerTree' - Before constructing each new tree\n",
      " |          - 'PerTreeLevel' - Before choosing each new split of a tree\n",
      " |  \n",
      " |  sampling_unit : string, [default='Object'].\n",
      " |      Possible values:\n",
      " |          - 'Object'\n",
      " |          - 'Group'\n",
      " |      The parameter allows to specify the sampling scheme:\n",
      " |      sample weights for each object individually or for an entire group of objects together.\n",
      " |  \n",
      " |  dev_score_calc_obj_block_size: int, [default=5000000]\n",
      " |      CPU only. Size of block of samples in score calculation. Should be > 0\n",
      " |      Used only for learning speed tuning.\n",
      " |      Changing this parameter can affect results due to numerical accuracy differences\n",
      " |  \n",
      " |  dev_efb_max_buckets : int, [default=1024]\n",
      " |      CPU only. Maximum bucket count in exclusive features bundle. Should be in an integer between 0 and 65536.\n",
      " |      Used only for learning speed tuning.\n",
      " |  \n",
      " |  sparse_features_conflict_fraction : float, [default=0.0]\n",
      " |      CPU only. Maximum allowed fraction of conflicting non-default values for features in exclusive features bundle.\n",
      " |      Should be a real value in [0, 1) interval.\n",
      " |  \n",
      " |  grow_policy : string, [SymmetricTree,Lossguide,Depthwise], [default=SymmetricTree]\n",
      " |      The tree growing policy. It describes how to perform greedy tree construction.\n",
      " |  \n",
      " |  min_data_in_leaf : int, [default=1].\n",
      " |      The minimum training samples count in leaf.\n",
      " |      CatBoost will not search for new splits in leaves with samples count less than min_data_in_leaf.\n",
      " |      This parameter is used only for Depthwise and Lossguide growing policies.\n",
      " |  \n",
      " |  max_leaves : int, [default=31],\n",
      " |      The maximum leaf count in resulting tree.\n",
      " |      This parameter is used only for Lossguide growing policy.\n",
      " |  \n",
      " |  score_function : string, possible values L2, Cosine, NewtonL2, NewtonCosine, [default=Cosine]\n",
      " |      For growing policy Lossguide default=NewtonL2.\n",
      " |      GPU only. Score that is used during tree construction to select the next tree split.\n",
      " |  \n",
      " |  max_depth : int, Synonym for depth.\n",
      " |  \n",
      " |  n_estimators : int, synonym for iterations.\n",
      " |  \n",
      " |  num_trees : int, synonym for iterations.\n",
      " |  \n",
      " |  num_boost_round : int, synonym for iterations.\n",
      " |  \n",
      " |  colsample_bylevel : float, synonym for rsm.\n",
      " |  \n",
      " |  random_state : int, synonym for random_seed.\n",
      " |  \n",
      " |  reg_lambda : float, synonym for l2_leaf_reg.\n",
      " |  \n",
      " |  objective : string, synonym for loss_function.\n",
      " |  \n",
      " |  num_leaves : int, synonym for max_leaves.\n",
      " |  \n",
      " |  min_child_samples : int, synonym for min_data_in_leaf\n",
      " |  \n",
      " |  eta : float, synonym for learning_rate.\n",
      " |  \n",
      " |  max_bin : float, synonym for border_count.\n",
      " |  \n",
      " |  scale_pos_weight : float, synonym for class_weights.\n",
      " |      Can be used only for binary classification. Sets weight multiplier for\n",
      " |      class 1 to scale_pos_weight value.\n",
      " |  \n",
      " |  metadata : dict, string to string key-value pairs to be stored in model metadata storage\n",
      " |  \n",
      " |  early_stopping_rounds : int\n",
      " |      Synonym for od_wait. Only one of these parameters should be set.\n",
      " |  \n",
      " |  cat_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Categ features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  text_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Text features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  embedding_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Embedding features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |  \n",
      " |  leaf_estimation_backtracking : string, [default=None]\n",
      " |      Type of backtracking during gradient descent.\n",
      " |      Possible values:\n",
      " |          - 'No' - never backtrack; supported on CPU and GPU\n",
      " |          - 'AnyImprovement' - reduce the descent step until the value of loss function is less than before the step; supported on CPU and GPU\n",
      " |          - 'Armijo' - reduce the descent step until Armijo condition is satisfied; supported on GPU only\n",
      " |  \n",
      " |  model_shrink_rate : float, [default=0]\n",
      " |      This parameter enables shrinkage of model at the start of each iteration. CPU only.\n",
      " |      For Constant mode shrinkage coefficient is calculated as (1 - model_shrink_rate * learning_rate).\n",
      " |      For Decreasing mode shrinkage coefficient is calculated as (1 - model_shrink_rate / iteration).\n",
      " |      Shrinkage coefficient should be in [0, 1).\n",
      " |  \n",
      " |  model_shrink_mode : string, [default=None]\n",
      " |      Mode of shrinkage coefficient calculation. CPU only.\n",
      " |      Possible values:\n",
      " |          - 'Constant' - Shrinkage coefficient is constant at each iteration.\n",
      " |          - 'Decreasing' - Shrinkage coefficient decreases at each iteration.\n",
      " |  \n",
      " |  langevin : bool, [default=False]\n",
      " |      Enables the Stochastic Gradient Langevin Boosting. CPU only.\n",
      " |  \n",
      " |  diffusion_temperature : float, [default=0]\n",
      " |      Langevin boosting diffusion temperature. CPU only.\n",
      " |  \n",
      " |  posterior_sampling : bool, [default=False]\n",
      " |      Set group of parameters for further use Uncertainty prediction:\n",
      " |          - Langevin = True\n",
      " |          - Model Shrink Rate = 1/(2N), where N is dataset size\n",
      " |          - Model Shrink Mode = Constant\n",
      " |          - Diffusion-temperature = N, where N is dataset size. CPU only.\n",
      " |  \n",
      " |  boost_from_average : bool, [default=True for RMSE, False for other losses]\n",
      " |      Enables to initialize approx values by best constant value for specified loss function.\n",
      " |      Available for RMSE, Logloss, CrossEntropy, Quantile and MAE.\n",
      " |  \n",
      " |  tokenizers : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'tokenizer_id': 'Tokenizer',  # Tokeinzer identifier.\n",
      " |              'lowercasing': 'false',  # Possible values: 'true', 'false'.\n",
      " |              'number_process_policy': 'LeaveAsIs',  # Possible values: 'Skip', 'LeaveAsIs', 'Replace'.\n",
      " |              'number_token': '%',  # Rarely used character. Used in conjunction with Replace NumberProcessPolicy.\n",
      " |              'separator_type': 'ByDelimiter',  # Possible values: 'ByDelimiter', 'BySense'.\n",
      " |              'delimiter': ' ',  # Used in conjunction with ByDelimiter SeparatorType.\n",
      " |              'split_by_set': 'false',  # Each single character in delimiter used as individual delimiter.\n",
      " |              'skip_empty': 'true',  # Possible values: 'true', 'false'.\n",
      " |              'token_types': ['Word', 'Number', 'Unknown'],  # Used in conjunction with BySense SeparatorType.\n",
      " |                  # Possible values: 'Word', 'Number', 'Punctuation', 'SentenceBreak', 'ParagraphBreak', 'Unknown'.\n",
      " |              'subtokens_policy': 'SingleToken',  # Possible values:\n",
      " |                  # 'SingleToken' - All subtokens are interpreted as single token).\n",
      " |                  # 'SeveralTokens' - All subtokens are interpreted as several token.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  dictionaries : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'dictionary_id': 'Dictionary',  # Dictionary identifier.\n",
      " |              'token_level_type': 'Word',  # Possible values: 'Word', 'Letter'.\n",
      " |              'gram_order': '1',  # 1 for Unigram, 2 for Bigram, ...\n",
      " |              'skip_step': '0',  # 1 for 1-skip-gram, ...\n",
      " |              'end_of_word_token_policy': 'Insert',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'end_of_sentence_token_policy': 'Skip',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'occurrence_lower_bound': '3',  # The lower bound of token occurrences in the text to include it in the dictionary.\n",
      " |              'max_dictionary_size': '50000',  # The max dictionary size.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  feature_calcers : list of strings,\n",
      " |      Each string is a calcer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          'NaiveBayes',\n",
      " |          'BM25',\n",
      " |          'BoW:top_tokens_count=2000',\n",
      " |      ]\n",
      " |      ```\n",
      " |  \n",
      " |  text_processing : dict,\n",
      " |      Text processging description.\n",
      " |  \n",
      " |  eval_fraction : float, [default=None]\n",
      " |      Fraction of the train dataset to be used as the evaluation dataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CatBoostClassifier\n",
      " |      CatBoost\n",
      " |      _CatBoostBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, random_score_type=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |      Initialize the CatBoost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for CatBoost.\n",
      " |          If  None, all params are set to their defaults.\n",
      " |          If  dict, overriding parameters present in dict.\n",
      " |  \n",
      " |  fit(self, X, y=None, cat_features=None, text_features=None, embedding_features=None, sample_weight=None, baseline=None, use_best_model=None, eval_set=None, verbose=None, logging_level=None, plot=False, plot_file=None, column_description=None, verbose_eval=None, metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, init_model=None, callbacks=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      Fit the CatBoostClassifier model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      cat_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Categ columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      text_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Text columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      embedding_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Embedding columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      sample_weight : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Instance weights, 1 dimensional array like.\n",
      " |      \n",
      " |      baseline : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving 2 dimensional array like data.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      use_best_model : bool, optional (default=None)\n",
      " |          Flag to use best model\n",
      " |      \n",
      " |      eval_set : catboost.Pool or list of catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Validation dataset or datasets for metrics calculation and possibly early stopping.\n",
      " |      \n",
      " |      metric_period : int\n",
      " |          Frequency of evaluating metrics.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      silent : bool\n",
      " |          If silent is True, logging_level is set to Silent.\n",
      " |          If silent is False, logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      verbose_eval : bool or int\n",
      " |          Synonym for verbose. Only one of these parameters should be set.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          Activates Iter overfitting detector with od_wait set to early_stopping_rounds.\n",
      " |      \n",
      " |      save_snapshot : bool, [default=None]\n",
      " |          Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |      \n",
      " |      snapshot_file : string or pathlib.Path, [default=None]\n",
      " |          Learn progress snapshot file path, if None will use default filename\n",
      " |      \n",
      " |      snapshot_interval: int, [default=600]\n",
      " |          Interval between saving snapshots (seconds)\n",
      " |      \n",
      " |      init_model : CatBoost class or string or pathlib.Path, [default=None]\n",
      " |          Continue training starting from the existing model.\n",
      " |          If this parameter is a string or pathlib.Path, load initial model from the path specified by this string.\n",
      " |      \n",
      " |      callbacks : list, optional (default=None)\n",
      " |          List of callback objects that are applied at end of each iteration.\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : CatBoost\n",
      " |  \n",
      " |  get_probability_threshold(self)\n",
      " |      Get a threshold for class separation in binary classification task\n",
      " |  \n",
      " |  predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return class label.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |  \n",
      " |  predict_log_proba(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class log probability with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |  \n",
      " |  predict_proba(self, X, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class probability with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If X is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If X is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Calculate accuracy.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          Data to apply model on.\n",
      " |      y : list or numpy.ndarray\n",
      " |          True labels.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy : float\n",
      " |  \n",
      " |  set_probability_threshold(self, binclass_probability_threshold=None)\n",
      " |      Set a threshold for class separation in binary classification task for a trained model.\n",
      " |      :param binclass_probability_threshold: float number in [0, 1] or None to discard it\n",
      " |  \n",
      " |  staged_predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return majority vote class.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |  \n",
      " |  staged_predict_log_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |  \n",
      " |  staged_predict_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CatBoost:\n",
      " |  \n",
      " |  calc_feature_statistics(self, data, target=None, feature=None, prediction_type=None, cat_feature_values=None, plot=True, max_cat_features_on_plot=10, thread_count=-1, plot_file=None)\n",
      " |      Get statistics for the feature using the model, dataset and target.\n",
      " |      To use this function, you should install plotly.\n",
      " |      \n",
      " |      The catboost model has borders for the float features used in it. The borders divide\n",
      " |      feature values into bins, and the model's prediction depends on the number of the bin where the\n",
      " |      feature value falls in.\n",
      " |      \n",
      " |      For float features this function takes model's borders and computes\n",
      " |      1) Mean target value for every bin;\n",
      " |      2) Mean model prediction for every bin;\n",
      " |      3) The number of objects in dataset which fall into each bin;\n",
      " |      4) Predictions on varying feature. For every object, varies the feature value\n",
      " |      so that it falls into bin #0, bin #1, ... and counts model predictions.\n",
      " |      Then counts average prediction for each bin.\n",
      " |      \n",
      " |      For categorical features (only one-hot supported) does the same, but takes feature values\n",
      " |      provided in cat_feature_values instead of borders.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost. Pool or dict {'pool_name': pool} if you want several pools\n",
      " |          Data to compute statistics on\n",
      " |      target: numpy.ndarray or pandas.Series or dict {'pool_name': target} if you want several pools or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      feature: None, int, string, or list of int or strings\n",
      " |          Features indexes or names in pd.DataFrame for which you want to get statistics.\n",
      " |          None, if you need statistics for all features.\n",
      " |      prediction_type: str\n",
      " |          Prediction type used for counting mean_prediction: 'Class', 'Probability' or 'RawFormulaVal'.\n",
      " |          If not specified, is derived from the model.\n",
      " |      cat_feature_values: list or numpy.ndarray or pandas.Series or\n",
      " |                          dict: int or string to list or numpy.ndarray or pandas.Series\n",
      " |          Contains categorical feature values you need to get statistics on.\n",
      " |          Use dict, when parameter 'feature' is a list to specify cat values for different features.\n",
      " |          When parameter 'feature' is int or str, you can just pass list of cat values.\n",
      " |      plot: bool\n",
      " |          Plot statistics.\n",
      " |      max_cat_features_on_plot: int\n",
      " |          If categorical feature takes more than max_cat_features_on_plot different unique values,\n",
      " |          output result on several plots, not more than max_cat_features_on_plot feature values on each.\n",
      " |          Used only if plot=True or plot_file is not None.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use for getting statistics.\n",
      " |      plot_file: str\n",
      " |          Output file for plot statistics.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict if parameter 'feature' is int or string, else dict of dicts:\n",
      " |          For each unique feature contain\n",
      " |          python dict with binarized feature statistics.\n",
      " |          For float feature, includes\n",
      " |                  'borders' -- borders for the specified feature in model\n",
      " |                  'binarized_feature' -- numbers of bins where feature values fall\n",
      " |                  'mean_target' -- mean value of target over each bin\n",
      " |                  'mean_prediction' -- mean value of model prediction over each bin\n",
      " |                  'objects_per_bin' -- number of objects per bin\n",
      " |                  'predictions_on_varying_feature' -- averaged over dataset predictions for\n",
      " |                  varying feature (see above)\n",
      " |          For one-hot feature, returns the same, but with 'cat_values' instead of 'borders'\n",
      " |  \n",
      " |  calc_leaf_indexes(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=False)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool (default=False)\n",
      " |          Enable debug logging level.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : 2-dimensional numpy.ndarray of numpy.uint32 with shape (object count, ntree_end - ntree_start).\n",
      " |          i-th row is an array of leaf indexes for i-th object.\n",
      " |  \n",
      " |  compare(self, model, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      Draw train and eval errors in Jupyter notebook for both models\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model: CatBoost model\n",
      " |          Another model to draw metrics\n",
      " |      \n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |  \n",
      " |  create_metric_calcer(self, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None)\n",
      " |      Create batch metric calcer. Could be used to aggregate metric on several pools\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |          Same as in eval_metrics except data\n",
      " |      Returns\n",
      " |      -------\n",
      " |          BatchMetricCalcer object\n",
      " |      \n",
      " |      Usage example\n",
      " |      -------\n",
      " |      # Large dataset is partitioned into parts [part1, part2]\n",
      " |      model.fit(params)\n",
      " |      batch_calcer = model.create_metric_calcer(['Logloss'])\n",
      " |      batch_calcer.add(part1)\n",
      " |      batch_calcer.add(part2)\n",
      " |      metrics = batch_calcer.eval_metrics()\n",
      " |  \n",
      " |  drop_unused_features(self)\n",
      " |      Drop unused features information from model\n",
      " |  \n",
      " |  eval_metrics(self, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      Calculate metrics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |      \n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : dict: metric -> array of shape [(ntree_end - ntree_start) / eval_period]\n",
      " |  \n",
      " |  get_all_params(self)\n",
      " |      Get all params (specified by user and default params) that were set in training from CatBoost model.\n",
      " |      Full parameters documentation could be found here: https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_borders(self)\n",
      " |      Return map feature_index: borders for float features.\n",
      " |  \n",
      " |  get_cat_feature_indices(self)\n",
      " |  \n",
      " |  get_embedding_feature_indices(self)\n",
      " |  \n",
      " |  get_feature_importance(self, data=None, type=<EFstrType.FeatureImportance: 2>, prettified=False, thread_count=-1, verbose=False, fstr_type=None, shap_mode='Auto', model_output='Raw', interaction_indices=None, shap_calc_type='Regular', reference_data=None, sage_n_samples=128, sage_batch_size=512, sage_detect_convergence=True, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          Data to get feature importance.\n",
      " |          If type in ('LossFunctionChange', 'ShapValues', 'ShapInteractionValues') data must of Pool type.\n",
      " |              For every object in this dataset feature importances will be calculated.\n",
      " |          if type == 'SageValues' data must of Pool type.\n",
      " |              For every feature in this dataset importance will be calculated.\n",
      " |          If type == 'PredictionValuesChange', data is None or a dataset of Pool type\n",
      " |              Dataset specification is needed only in case if the model does not contain leaf weight information (trained with CatBoost v < 0.9).\n",
      " |          If type == 'PredictionDiff' data must contain a matrix of feature values of shape (2, n_features).\n",
      " |              Possible types are catboost.Pool or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData or pandas.SparseDataFrame or scipy.sparse.spmatrix\n",
      " |          If type == 'FeatureImportance'\n",
      " |              See 'PredictionValuesChange' for non-ranking metrics and 'LossFunctionChange' for ranking metrics.\n",
      " |          If type == 'Interaction'\n",
      " |              This parameter is not used.\n",
      " |      \n",
      " |      type : EFstrType or string (converted to EFstrType), optional\n",
      " |                  (default=EFstrType.FeatureImportance)\n",
      " |          Possible values:\n",
      " |              - PredictionValuesChange\n",
      " |                  Calculate score for every feature.\n",
      " |              - LossFunctionChange\n",
      " |                  Calculate score for every feature by loss.\n",
      " |              - FeatureImportance\n",
      " |                  PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics\n",
      " |              - ShapValues\n",
      " |                  Calculate SHAP Values for every object.\n",
      " |              - ShapInteractionValues\n",
      " |                  Calculate SHAP Interaction Values between each pair of features for every object\n",
      " |              - Interaction\n",
      " |                  Calculate pairwise score between every feature.\n",
      " |              - PredictionDiff\n",
      " |                  Calculate most important features explaining difference in predictions for a pair of documents.\n",
      " |              - SageValues\n",
      " |                  Calculate SAGE value for every feature\n",
      " |      \n",
      " |      prettified : bool, optional (default=False)\n",
      " |          change returned data format to the list of (feature_id, importance) pairs sorted by importance\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      fstr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      shap_mode : string, optional (default=\"Auto\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Auto\"\n",
      " |                  Use direct SHAP Values calculation only if data size is smaller than average leaves number\n",
      " |                  (the best of two strategies below is chosen).\n",
      " |              - \"UsePreCalc\"\n",
      " |                  Calculate SHAP Values for every leaf in preprocessing. Final complexity is\n",
      " |                  O(NT(D+F))+O(TL^2 D^2) where N is the number of documents(objects), T - number of trees,\n",
      " |                  D - average tree depth, F - average number of features in tree, L - average number of leaves in tree\n",
      " |                  This is much faster (because of a smaller constant) than direct calculation when N >> L\n",
      " |              - \"NoPreCalc\"\n",
      " |                  Use direct SHAP Values calculation calculation with complexity O(NTLD^2). Direct algorithm\n",
      " |                  is faster when N < L (algorithm from https://arxiv.org/abs/1802.03888)\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=\"Regular\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Regular\"\n",
      " |                  Calculate regular SHAP values\n",
      " |              - \"Approximate\"\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - \"Exact\"\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      interaction_indices : list of int or string (feature_idx_1, feature_idx_2), optional (default=None)\n",
      " |          used only for ShapInteractionValues type\n",
      " |          Calculate SHAP Interaction Values between pair of features feature_idx_1 and feature_idx_2 for every object\n",
      " |      \n",
      " |      reference_data: catboost.Pool or None\n",
      " |          Reference data for Independent Tree SHAP values from https://arxiv.org/abs/1905.04610v1\n",
      " |          if type == 'ShapValues' and reference_data is not None, then Independent Tree SHAP values are calculated\n",
      " |      \n",
      " |      sage_n_samples: int, optional (default=32)\n",
      " |          Number of outer samples used in SAGE values approximation algorithm\n",
      " |      sage_batch_size: int, optional (default=min(512, number of samples in dataset))\n",
      " |          Number of samples used on each step of SAGE values approximation algorithm\n",
      " |      sage_detect_convergence: bool, optional (default=False)\n",
      " |          If set True, sage values calculation will be stopped either when sage values converge\n",
      " |          or when sage_n_samples iterations of algorithm pass\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      depends on type:\n",
      " |          - FeatureImportance\n",
      " |              See PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics.\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=False (default)\n",
      " |              list of length [n_features] with feature_importance values (float) for feature\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=True\n",
      " |              list of length [n_features] with (feature_id (string), feature_importance (float)) pairs, sorted by feature_importance in descending order\n",
      " |          - ShapValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1) with Shap values (float) for (object, feature).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1). For each object it contains Shap values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - ShapInteractionValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1, n_features + 1) with Shap interaction values (float) for (object, feature(i), feature(j)).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1, n_features + 1). For each object it contains Shap interaction values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - Interaction\n",
      " |              list of length [n_features] of 3-element lists of (first_feature_index, second_feature_index, interaction_score (float))\n",
      " |  \n",
      " |  get_object_importance(self, pool, train_pool, top_size=-1, type='Average', update_method='SinglePoint', importance_values_sign='All', thread_count=-1, verbose=False, ostr_type=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      This is the implementation of the LeafInfluence algorithm from the following paper:\n",
      " |      https://arxiv.org/pdf/1802.06640.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pool : Pool\n",
      " |          The pool for which you want to evaluate the object importances.\n",
      " |      \n",
      " |      train_pool : Pool\n",
      " |          The pool on which the model has been trained.\n",
      " |      \n",
      " |      top_size : int (default=-1)\n",
      " |          Method returns the result of the top_size most important train objects.\n",
      " |          If -1, then the top size is not limited.\n",
      " |      \n",
      " |      type : string, optional (default='Average')\n",
      " |          Possible values:\n",
      " |              - Average (Method returns the mean train objects scores for all input objects)\n",
      " |              - PerObject (Method returns the train objects scores for every input object)\n",
      " |      \n",
      " |      importance_values_sign : string, optional (default='All')\n",
      " |          Method returns only Positive, Negative or All values.\n",
      " |          Possible values:\n",
      " |              - Positive\n",
      " |              - Negative\n",
      " |              - All\n",
      " |      \n",
      " |      update_method : string, optional (default='SinglePoint')\n",
      " |          Possible values:\n",
      " |              - SinglePoint\n",
      " |              - TopKLeaves (It is posible to set top size : TopKLeaves:top=2)\n",
      " |              - AllPoints\n",
      " |          Description of the update set methods are given in section 3.1.3 of the paper.\n",
      " |      \n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |      \n",
      " |      ostr_type : string, deprecated, use type instead\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object_importances : tuple of two arrays (indices and scores) of shape = [top_size]\n",
      " |  \n",
      " |  get_param(self, key)\n",
      " |      Get param value from CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : string\n",
      " |          The key to get param value from.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      value :\n",
      " |          The param value of the key, returns None if param do not exist.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get all params from CatBoost model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |  \n",
      " |  get_text_feature_indices(self)\n",
      " |  \n",
      " |  grid_search(self, param_grid, X, y=None, cv=3, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      Exhaustive search over specified parameter values for a model.\n",
      " |      Aafter calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_grid: dict or list of dictionaries\n",
      " |          Dictionary with parameters names (string) as keys and lists of parameter settings\n",
      " |          to try as values, or a list of such dictionaries, in which case the grids spanned by each\n",
      " |          dictionary in the list are explored.\n",
      " |          This enables searching over any sequence of parameter settings.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for final cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  iterate_leaf_indexes(self, data, ntree_start=0, ntree_end=0)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : generator. For each object in pool yields one-dimensional numpy.ndarray of leaf indexes.\n",
      " |  \n",
      " |  load_model(self, fname=None, format='cbm', stream=None, blob=None)\n",
      " |      Load model from a file, stream or blob.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |  \n",
      " |  plot_partial_dependence(self, data, features, plot=True, plot_file=None, thread_count=-1)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      features: int, str, list<int>, tuple<int>, list<string>, tuple<string>\n",
      " |          Float features to calculate partial dependence for. Number of features should be 1 or 2.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use. If -1 use maximum available number of threads.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          If number of features is one - 1d numpy array and figure with line plot.\n",
      " |          If number of features is two - 2d numpy array and figure with 2d heatmap.\n",
      " |  \n",
      " |  plot_predictions(self, data, features_to_change, plot=True, plot_file=None)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      feature:\n",
      " |          Float features indexes in pd.DataFrame for which you want vary prediction value.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          List of list of predictions for all buckets for all documents in data\n",
      " |  \n",
      " |  plot_tree(self, tree_idx, pool=None)\n",
      " |  \n",
      " |  randomized_search(self, param_distributions, X, y=None, cv=3, n_iter=10, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>)\n",
      " |      Randomized search on hyper parameters.\n",
      " |      After calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |      \n",
      " |      In contrast to grid_search, not all parameter values are tried out,\n",
      " |      but rather a fixed number of parameter settings is sampled from the specified distributions.\n",
      " |      The number of parameter settings that are tried is given by n_iter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_distributions: dict\n",
      " |          Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.\n",
      " |          Distributions must provide a rvs method for sampling (such as those from scipy.stats.distributions).\n",
      " |          If a list is given, it is sampled uniformly.\n",
      " |      \n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |      \n",
      " |      y: numpy.ndarray or pandas.Series or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      \n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |      \n",
      " |      n_iter: int\n",
      " |          Number of parameter settings that are sampled.\n",
      " |          n_iter trades off runtime vs quality of the solution.\n",
      " |      \n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |      \n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |      \n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |      \n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |      \n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |      \n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |      \n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for cross-validation.\n",
      " |      \n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |      \n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |  \n",
      " |  save_borders(self, fname)\n",
      " |      Save the model borders to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or pathlib.Path\n",
      " |          Output file name.\n",
      " |  \n",
      " |  save_model(self, fname, format='cbm', export_parameters=None, pool=None)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name.\n",
      " |      format : string\n",
      " |          Possible values:\n",
      " |              * 'cbm' for catboost binary format,\n",
      " |              * 'coreml' to export into Apple CoreML format\n",
      " |              * 'onnx' to export into ONNX-ML format\n",
      " |              * 'pmml' to export into PMML format\n",
      " |              * 'cpp' to export as C++ code\n",
      " |              * 'python' to export as Python code.\n",
      " |      export_parameters : dict\n",
      " |          Parameters for CoreML export:\n",
      " |              * prediction_type : string - either 'probability' or 'raw'\n",
      " |              * coreml_description : string\n",
      " |              * coreml_model_version : string\n",
      " |              * coreml_model_author : string\n",
      " |              * coreml_model_license: string\n",
      " |          Parameters for PMML export:\n",
      " |              * pmml_copyright : string\n",
      " |              * pmml_description : string\n",
      " |              * pmml_model_version : string\n",
      " |      pool : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series or catboost.FeaturesData\n",
      " |          Training pool.\n",
      " |  \n",
      " |  select_features(self, X, y=None, eval_set=None, features_for_select=None, num_features_to_select=None, algorithm=None, steps=None, shap_calc_type=None, train_final_model=True, verbose=None, logging_level=None, plot=False, plot_file=None, log_cout=<ipykernel.iostream.OutStream object at 0x105df5f30>, log_cerr=<ipykernel.iostream.OutStream object at 0x105df5f90>, grouping=None, features_tags_for_select=None, num_features_tags_to_select=None)\n",
      " |      Select best features from pool according to loss value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |      \n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels, 1 dimensional array like.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |      \n",
      " |      eval_set : catboost.Pool or list of catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Validation dataset or datasets for metrics calculation and possibly early stopping.\n",
      " |      \n",
      " |      features_for_select : str or list of feature indices, names or ranges\n",
      " |          (for grouping = Individual)\n",
      " |          Which features should participate in the selection.\n",
      " |          Format examples:\n",
      " |              - [0, 2, 3, 4, 17]\n",
      " |              - [0, \"2-4\", 17] (both ends in ranges are inclusive)\n",
      " |              - \"0,2-4,20\"\n",
      " |              - [\"Name0\", \"Name2\", \"Name3\", \"Name4\", \"Name20\"]\n",
      " |      \n",
      " |      num_features_to_select : positive int\n",
      " |          (for grouping = Individual)\n",
      " |          How many features to select from features_for_select.\n",
      " |      \n",
      " |      algorithm : EFeaturesSelectionAlgorithm or string, optional (default=RecursiveByShapValues)\n",
      " |          Which algorithm to use for features selection.\n",
      " |          Possible values:\n",
      " |              - RecursiveByPredictionValuesChange\n",
      " |                  Use prediction values change as feature strength, eliminate batch of features at once.\n",
      " |              - RecursiveByLossFunctionChange\n",
      " |                  Use loss function change as feature strength, eliminate batch of features at each step.\n",
      " |              - RecursiveByShapValues\n",
      " |                  Use shap values to estimate loss function change, eliminate features one by one.\n",
      " |      \n",
      " |      steps : positive int, optional (default=1)\n",
      " |          How many steps should be performed. In other words, how many times a full model will be trained.\n",
      " |          More steps give more accurate results.\n",
      " |      \n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=Regular)\n",
      " |          Which method to use for calculation of shap values.\n",
      " |          Possible values:\n",
      " |              - Regular\n",
      " |                  Calculate regular SHAP values\n",
      " |              - Approximate\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - Exact\n",
      " |                  Calculate exact SHAP values\n",
      " |      \n",
      " |      train_final_model : bool, optional (default=True)\n",
      " |          Need to fit model with selected features.\n",
      " |      \n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |      \n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |      \n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook.\n",
      " |      \n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |      \n",
      " |      log_cout: output stream or callback for logging\n",
      " |      \n",
      " |      log_cerr: error stream or callback for logging\n",
      " |      \n",
      " |      grouping : EFeaturesSelectionGrouping or string, optional (default=Individual)\n",
      " |          Which grouping to use for features selection.\n",
      " |          Possible values:\n",
      " |              - Individual\n",
      " |                  Select individual features\n",
      " |              - ByTags\n",
      " |                  Select feature groups (marked by tags)\n",
      " |      \n",
      " |      features_tags_for_select : list of strings\n",
      " |          (for grouping = ByTags)\n",
      " |          Which features tags should participate in the selection.\n",
      " |      \n",
      " |      num_features_tags_to_select : positive int\n",
      " |          (for grouping = ByTags)\n",
      " |          How many features tags to select from features_tags_for_select.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with fields:\n",
      " |          'selected_features': list of selected features indices\n",
      " |          'eliminated_features': list of eliminated features indices\n",
      " |          'selected_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |          'eliminated_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set parameters into CatBoost model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : key=value format\n",
      " |          List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\n",
      " |  \n",
      " |  shrink(self, ntree_end, ntree_start=0)\n",
      " |      Shrink the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ntree_end: int\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |  \n",
      " |  virtual_ensembles_predict(self, data, prediction_type='VirtEnsembles', ntree_end=0, virtual_ensembles_count=10, thread_count=-1, verbose=None)\n",
      " |      Predict with data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |      \n",
      " |      prediction_type : string, optional (default='RawFormulaVal')\n",
      " |          Can be:\n",
      " |          - 'VirtEnsembles': return V (virtual_ensembles_count) predictions.\n",
      " |              k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant.\n",
      " |          - 'TotalUncertainty': see returned predictions format in 'Returns' part\n",
      " |      \n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |      \n",
      " |      virtual_ensembles_count: int, optional (default=10)\n",
      " |          virtual ensembles count for 'TotalUncertainty' and 'VirtEnsembles' prediction types.\n",
      " |      \n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |      \n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          (with V as virtual_ensembles_count and T as trees count,\n",
      " |          k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant)\n",
      " |          If data is for a single object, return 1-dimensional array of predictions with size depends on prediction type,\n",
      " |          otherwise return 2-dimensional numpy.ndarray with shape (number_of_objects x size depends on prediction type);\n",
      " |          Returned predictions depends on prediction type:\n",
      " |          If loss-function was RMSEWithUncertainty:\n",
      " |              - 'VirtEnsembles': [mean0, var0, mean1, var1, ..., vark-1].\n",
      " |              - 'TotalUncertainty': [mean_predict, KnowledgeUnc, DataUnc].\n",
      " |          otherwise for regression:\n",
      " |              - 'VirtEnsembles':  [mean0, mean1, ...].\n",
      " |              - 'TotalUncertainty': [mean_predicts, KnowledgeUnc].\n",
      " |          otherwise for binary classification:\n",
      " |              - 'VirtEnsembles':  [ApproxRawFormulaVal0, ApproxRawFormulaVal1, ..., ApproxRawFormulaValk-1].\n",
      " |              - 'TotalUncertainty':  [DataUnc, TotalUnc].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from CatBoost:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |  \n",
      " |  __deepcopy__(self, _)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  copy(self)\n",
      " |  \n",
      " |  get_best_iteration(self)\n",
      " |  \n",
      " |  get_best_score(self)\n",
      " |  \n",
      " |  get_evals_result(self)\n",
      " |  \n",
      " |  get_leaf_values(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_values : 1d-array of leaf values for all trees.\n",
      " |      Value corresponding to j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_leaf_weights(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_weights : 1d-array of leaf weights for all trees.\n",
      " |      Weight of j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  get_metadata(self)\n",
      " |  \n",
      " |  get_n_features_in(self)\n",
      " |  \n",
      " |  get_scale_and_bias(self)\n",
      " |  \n",
      " |  get_test_eval(self)\n",
      " |  \n",
      " |  get_test_evals(self)\n",
      " |  \n",
      " |  get_tree_leaf_counts(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tree_leaf_counts : 1d-array of numpy.uint32 of size tree_count_.\n",
      " |      tree_leaf_counts[i] equals to the number of leafs in i-th tree of the ensemble.\n",
      " |  \n",
      " |  is_fitted(self)\n",
      " |  \n",
      " |  set_feature_names(self, feature_names)\n",
      " |      Sets feature names equal to feature_names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature_names: 1-d array of strings with new feature names in the same order as in pool\n",
      " |  \n",
      " |  set_leaf_values(self, new_leaf_values)\n",
      " |      Sets values at tree leafs of ensemble equal to new_leaf_values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_leaf_values : 1d-array with new leaf values for all trees.\n",
      " |      It's size should be equal to sum(get_tree_leaf_counts()).\n",
      " |      Value corresponding to j-th leaf of i-th tree should be at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |  \n",
      " |  set_scale_and_bias(self, scale, bias)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _CatBoostBase:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |  \n",
      " |  best_score_\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  evals_result_\n",
      " |  \n",
      " |  feature_names_\n",
      " |  \n",
      " |  learning_rate_\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  random_seed_\n",
      " |  \n",
      " |  tree_count_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CatBoostBase:\n",
      " |  \n",
      " |  __hash__ = None\n"
     ]
    }
   ],
   "source": [
    "help(CatBoostClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:41:26.127899Z",
     "start_time": "2023-09-28T13:41:26.096487Z"
    }
   },
   "id": "cfe68581c4a868d2"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:55:23.528943Z",
     "start_time": "2023-09-28T13:55:23.500610Z"
    }
   },
   "id": "3aaeebf141bfb6c0"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LGBMClassifier in module lightgbm.sklearn:\n",
      "\n",
      "class LGBMClassifier(sklearn.base.ClassifierMixin, LGBMModel)\n",
      " |  LGBMClassifier(boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[str, Callable[[Optional[numpy.ndarray], numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = None, class_weight: Union[Dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: Optional[int] = None, importance_type: str = 'split', **kwargs)\n",
      " |  \n",
      " |  LightGBM classifier.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LGBMClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      LGBMModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, X: Union[lightgbm.compat.dt_DataTable, List[Union[List[float], List[int]]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse._matrix.spmatrix], y: Union[List[float], List[int], numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame], sample_weight: Union[List[float], List[int], numpy.ndarray, pandas.core.series.Series, NoneType] = None, init_score: Union[List[float], List[List[float]], numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame, NoneType] = None, eval_set: Optional[List[Tuple[Union[lightgbm.compat.dt_DataTable, List[Union[List[float], List[int]]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse._matrix.spmatrix], Union[List[float], List[int], numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame]]]] = None, eval_names: Optional[List[str]] = None, eval_sample_weight: Optional[List[Union[List[float], List[int], numpy.ndarray, pandas.core.series.Series]]] = None, eval_class_weight: Optional[List[float]] = None, eval_init_score: Optional[List[Union[List[float], List[List[float]], numpy.ndarray, pandas.core.series.Series, pandas.core.frame.DataFrame]]] = None, eval_metric: Union[str, Callable[[Optional[numpy.ndarray], numpy.ndarray], Tuple[str, float, bool]], Callable[[Optional[numpy.ndarray], numpy.ndarray], List[Tuple[str, float, bool]]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], Tuple[str, float, bool]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], List[Tuple[str, float, bool]]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], Tuple[str, float, bool]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], List[Tuple[str, float, bool]]], List[Union[str, Callable[[Optional[numpy.ndarray], numpy.ndarray], Tuple[str, float, bool]], Callable[[Optional[numpy.ndarray], numpy.ndarray], List[Tuple[str, float, bool]]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], Tuple[str, float, bool]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], List[Tuple[str, float, bool]]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], Tuple[str, float, bool]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], List[Tuple[str, float, bool]]]]], NoneType] = None, feature_name: Union[List[str], ForwardRef(\"Literal['auto']\")] = 'auto', categorical_feature: Union[List[str], List[int], ForwardRef(\"Literal['auto']\")] = 'auto', callbacks: Optional[List[Callable]] = None, init_model: Union[str, pathlib.Path, lightgbm.basic.Booster, lightgbm.sklearn.LGBMModel, NoneType] = None) -> 'LGBMClassifier'\n",
      " |      Build a gradient boosting model from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array, pandas DataFrame, H2O DataTable's Frame , scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\n",
      " |          Input feature matrix.\n",
      " |      y : numpy array, pandas DataFrame, pandas Series, list of int or float of shape = [n_samples]\n",
      " |          The target values (class labels in classification, real numbers in regression).\n",
      " |      sample_weight : numpy array, pandas Series, list of int or float of shape = [n_samples] or None, optional (default=None)\n",
      " |          Weights of training data. Weights should be non-negative.\n",
      " |      init_score : numpy array, pandas DataFrame, pandas Series, list of int or float of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task) or shape = [n_samples, n_classes] (for multi-class task) or None, optional (default=None)\n",
      " |          Init score of training data.\n",
      " |      eval_set : list or None, optional (default=None)\n",
      " |          A list of (X, y) tuple pairs to use as validation sets.\n",
      " |      eval_names : list of str, or None, optional (default=None)\n",
      " |          Names of eval_set.\n",
      " |      eval_sample_weight : list of array (same types as ``sample_weight`` supports), or None, optional (default=None)\n",
      " |          Weights of eval data. Weights should be non-negative.\n",
      " |      eval_class_weight : list or None, optional (default=None)\n",
      " |          Class weights of eval data.\n",
      " |      eval_init_score : list of array (same types as ``init_score`` supports), or None, optional (default=None)\n",
      " |          Init score of eval data.\n",
      " |      eval_metric : str, callable, list or None, optional (default=None)\n",
      " |          If str, it should be a built-in evaluation metric to use.\n",
      " |          If callable, it should be a custom evaluation metric, see note below for more details.\n",
      " |          If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both.\n",
      " |          In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n",
      " |          Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n",
      " |      feature_name : list of str, or 'auto', optional (default='auto')\n",
      " |          Feature names.\n",
      " |          If 'auto' and data is pandas DataFrame, data columns names are used.\n",
      " |      categorical_feature : list of str or int, or 'auto', optional (default='auto')\n",
      " |          Categorical features.\n",
      " |          If list of int, interpreted as indices.\n",
      " |          If list of str, interpreted as feature names (need to specify ``feature_name`` as well).\n",
      " |          If 'auto' and data is pandas DataFrame, pandas unordered categorical columns are used.\n",
      " |          All values in categorical features will be cast to int32 and thus should be less than int32 max value (2147483647).\n",
      " |          Large values could be memory consuming. Consider using consecutive integers starting from zero.\n",
      " |          All negative values in categorical features will be treated as missing values.\n",
      " |          The output cannot be monotonically constrained with respect to a categorical feature.\n",
      " |          Floating point numbers in categorical features will be rounded towards 0.\n",
      " |      callbacks : list of callable, or None, optional (default=None)\n",
      " |          List of callback functions that are applied at each iteration.\n",
      " |          See Callbacks in Python API for more information.\n",
      " |      init_model : str, pathlib.Path, Booster, LGBMModel or None, optional (default=None)\n",
      " |          Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : LGBMClassifier\n",
      " |          Returns self.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      Custom eval function expects a callable with following signatures:\n",
      " |      ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n",
      " |      ``func(y_true, y_pred, weight, group)``\n",
      " |      and returns (eval_name, eval_result, is_higher_better) or\n",
      " |      list of (eval_name, eval_result, is_higher_better):\n",
      " |      \n",
      " |          y_true : numpy 1-D array of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |              In case of custom ``objective``, predicted values are returned before any transformation,\n",
      " |              e.g. they are raw margin instead of probability of positive class for binary task in this case.\n",
      " |          weight : numpy 1-D array of shape = [n_samples]\n",
      " |              The weight of samples. Weights should be non-negative.\n",
      " |          group : numpy 1-D array\n",
      " |              Group/query data.\n",
      " |              Only used in the learning-to-rank task.\n",
      " |              sum(group) = n_samples.\n",
      " |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      " |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      " |          eval_name : str\n",
      " |              The name of evaluation function (without whitespace).\n",
      " |          eval_result : float\n",
      " |              The eval result.\n",
      " |          is_higher_better : bool\n",
      " |              Is eval result higher better, e.g. AUC is ``is_higher_better``.\n",
      " |  \n",
      " |  predict(self, X: Union[lightgbm.compat.dt_DataTable, List[Union[List[float], List[int]]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse._matrix.spmatrix], raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any)\n",
      " |      Return the predicted value for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array, pandas DataFrame, H2O DataTable's Frame , scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      start_iteration : int, optional (default=0)\n",
      " |          Start index of the iteration to predict.\n",
      " |          If <= 0, starts from the first iteration.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Total number of iterations used in the prediction.\n",
      " |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      " |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      " |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      " |              like SHAP interaction values,\n",
      " |              you can install the shap package (https://github.com/slundberg/shap).\n",
      " |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      " |              column, where the last column is the expected value.\n",
      " |      \n",
      " |      validate_features : bool, optional (default=False)\n",
      " |          If True, ensure that the features used to predict match the ones used to train.\n",
      " |          Used only if data is pandas DataFrame.\n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      " |          The predicted values.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      " |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      " |  \n",
      " |  predict_proba(self, X: Union[lightgbm.compat.dt_DataTable, List[Union[List[float], List[int]]], numpy.ndarray, pandas.core.frame.DataFrame, scipy.sparse._matrix.spmatrix], raw_score: bool = False, start_iteration: int = 0, num_iteration: Optional[int] = None, pred_leaf: bool = False, pred_contrib: bool = False, validate_features: bool = False, **kwargs: Any)\n",
      " |      Return the predicted probability for each class for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array, pandas DataFrame, H2O DataTable's Frame , scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      raw_score : bool, optional (default=False)\n",
      " |          Whether to predict raw scores.\n",
      " |      start_iteration : int, optional (default=0)\n",
      " |          Start index of the iteration to predict.\n",
      " |          If <= 0, starts from the first iteration.\n",
      " |      num_iteration : int or None, optional (default=None)\n",
      " |          Total number of iterations used in the prediction.\n",
      " |          If None, if the best iteration exists and start_iteration <= 0, the best iteration is used;\n",
      " |          otherwise, all iterations from ``start_iteration`` are used (no limits).\n",
      " |          If <= 0, all iterations from ``start_iteration`` are used (no limits).\n",
      " |      pred_leaf : bool, optional (default=False)\n",
      " |          Whether to predict leaf index.\n",
      " |      pred_contrib : bool, optional (default=False)\n",
      " |          Whether to predict feature contributions.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If you want to get more explanations for your model's predictions using SHAP values,\n",
      " |              like SHAP interaction values,\n",
      " |              you can install the shap package (https://github.com/slundberg/shap).\n",
      " |              Note that unlike the shap package, with ``pred_contrib`` we return a matrix with an extra\n",
      " |              column, where the last column is the expected value.\n",
      " |      \n",
      " |      validate_features : bool, optional (default=False)\n",
      " |          If True, ensure that the features used to predict match the ones used to train.\n",
      " |          Used only if data is pandas DataFrame.\n",
      " |      **kwargs\n",
      " |          Other parameters for the prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predicted_probability : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n",
      " |          The predicted values.\n",
      " |      X_leaves : array-like of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]\n",
      " |          If ``pred_leaf=True``, the predicted leaf of every tree for each sample.\n",
      " |      X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes] or list with n_classes length of such objects\n",
      " |          If ``pred_contrib=True``, the feature contributions for each sample.\n",
      " |  \n",
      " |  set_fit_request(self: lightgbm.sklearn.LGBMClassifier, *, callbacks: Union[bool, NoneType, str] = '$UNCHANGED$', categorical_feature: Union[bool, NoneType, str] = '$UNCHANGED$', eval_class_weight: Union[bool, NoneType, str] = '$UNCHANGED$', eval_init_score: Union[bool, NoneType, str] = '$UNCHANGED$', eval_metric: Union[bool, NoneType, str] = '$UNCHANGED$', eval_names: Union[bool, NoneType, str] = '$UNCHANGED$', eval_sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_name: Union[bool, NoneType, str] = '$UNCHANGED$', init_model: Union[bool, NoneType, str] = '$UNCHANGED$', init_score: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> lightgbm.sklearn.LGBMClassifier\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      callbacks : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``callbacks`` parameter in ``fit``.\n",
      " |      \n",
      " |      categorical_feature : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``categorical_feature`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_class_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_class_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_init_score : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_init_score`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_metric : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_metric`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_names : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_names`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
      " |      \n",
      " |      feature_name : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``feature_name`` parameter in ``fit``.\n",
      " |      \n",
      " |      init_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``init_model`` parameter in ``fit``.\n",
      " |      \n",
      " |      init_score : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``init_score`` parameter in ``fit``.\n",
      " |      \n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_predict_proba_request(self: lightgbm.sklearn.LGBMClassifier, *, num_iteration: Union[bool, NoneType, str] = '$UNCHANGED$', pred_contrib: Union[bool, NoneType, str] = '$UNCHANGED$', pred_leaf: Union[bool, NoneType, str] = '$UNCHANGED$', raw_score: Union[bool, NoneType, str] = '$UNCHANGED$', start_iteration: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> lightgbm.sklearn.LGBMClassifier\n",
      " |      Request metadata passed to the ``predict_proba`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_iteration : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``num_iteration`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      pred_contrib : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``pred_contrib`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      pred_leaf : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``pred_leaf`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      raw_score : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_score`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      start_iteration : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``start_iteration`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``validate_features`` parameter in ``predict_proba``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_predict_request(self: lightgbm.sklearn.LGBMClassifier, *, num_iteration: Union[bool, NoneType, str] = '$UNCHANGED$', pred_contrib: Union[bool, NoneType, str] = '$UNCHANGED$', pred_leaf: Union[bool, NoneType, str] = '$UNCHANGED$', raw_score: Union[bool, NoneType, str] = '$UNCHANGED$', start_iteration: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> lightgbm.sklearn.LGBMClassifier\n",
      " |      Request metadata passed to the ``predict`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num_iteration : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``num_iteration`` parameter in ``predict``.\n",
      " |      \n",
      " |      pred_contrib : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``pred_contrib`` parameter in ``predict``.\n",
      " |      \n",
      " |      pred_leaf : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``pred_leaf`` parameter in ``predict``.\n",
      " |      \n",
      " |      raw_score : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_score`` parameter in ``predict``.\n",
      " |      \n",
      " |      start_iteration : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``start_iteration`` parameter in ``predict``.\n",
      " |      \n",
      " |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: lightgbm.sklearn.LGBMClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> lightgbm.sklearn.LGBMClassifier\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |      :obj:`array` of shape = [n_classes]: The class label array.\n",
      " |  \n",
      " |  n_classes_\n",
      " |      :obj:`int`: The number of classes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LGBMModel:\n",
      " |  \n",
      " |  __init__(self, boosting_type: str = 'gbdt', num_leaves: int = 31, max_depth: int = -1, learning_rate: float = 0.1, n_estimators: int = 100, subsample_for_bin: int = 200000, objective: Union[str, Callable[[Optional[numpy.ndarray], numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], Callable[[Optional[numpy.ndarray], numpy.ndarray, Optional[numpy.ndarray], Optional[numpy.ndarray]], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = None, class_weight: Union[Dict, str, NoneType] = None, min_split_gain: float = 0.0, min_child_weight: float = 0.001, min_child_samples: int = 20, subsample: float = 1.0, subsample_freq: int = 0, colsample_bytree: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 0.0, random_state: Union[int, numpy.random.mtrand.RandomState, NoneType] = None, n_jobs: Optional[int] = None, importance_type: str = 'split', **kwargs)\n",
      " |      Construct a gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      boosting_type : str, optional (default='gbdt')\n",
      " |          'gbdt', traditional Gradient Boosting Decision Tree.\n",
      " |          'dart', Dropouts meet Multiple Additive Regression Trees.\n",
      " |          'rf', Random Forest.\n",
      " |      num_leaves : int, optional (default=31)\n",
      " |          Maximum tree leaves for base learners.\n",
      " |      max_depth : int, optional (default=-1)\n",
      " |          Maximum tree depth for base learners, <=0 means no limit.\n",
      " |      learning_rate : float, optional (default=0.1)\n",
      " |          Boosting learning rate.\n",
      " |          You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n",
      " |          in training using ``reset_parameter`` callback.\n",
      " |          Note, that this will ignore the ``learning_rate`` argument in training.\n",
      " |      n_estimators : int, optional (default=100)\n",
      " |          Number of boosted trees to fit.\n",
      " |      subsample_for_bin : int, optional (default=200000)\n",
      " |          Number of samples for constructing bins.\n",
      " |      objective : str, callable or None, optional (default=None)\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |          Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
      " |      class_weight : dict, 'balanced' or None, optional (default=None)\n",
      " |          Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |          Use this parameter only for multi-class classification task;\n",
      " |          for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n",
      " |          Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n",
      " |          You may want to consider performing probability calibration\n",
      " |          (https://scikit-learn.org/stable/modules/calibration.html) of your model.\n",
      " |          The 'balanced' mode uses the values of y to automatically adjust weights\n",
      " |          inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |          If None, all classes are supposed to have weight one.\n",
      " |          Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n",
      " |          if ``sample_weight`` is specified.\n",
      " |      min_split_gain : float, optional (default=0.)\n",
      " |          Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |      min_child_weight : float, optional (default=1e-3)\n",
      " |          Minimum sum of instance weight (Hessian) needed in a child (leaf).\n",
      " |      min_child_samples : int, optional (default=20)\n",
      " |          Minimum number of data needed in a child (leaf).\n",
      " |      subsample : float, optional (default=1.)\n",
      " |          Subsample ratio of the training instance.\n",
      " |      subsample_freq : int, optional (default=0)\n",
      " |          Frequency of subsample, <=0 means no enable.\n",
      " |      colsample_bytree : float, optional (default=1.)\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      reg_alpha : float, optional (default=0.)\n",
      " |          L1 regularization term on weights.\n",
      " |      reg_lambda : float, optional (default=0.)\n",
      " |          L2 regularization term on weights.\n",
      " |      random_state : int, RandomState object or None, optional (default=None)\n",
      " |          Random number seed.\n",
      " |          If int, this number is used to seed the C++ code.\n",
      " |          If RandomState object (numpy), a random integer is picked based on its state to seed the C++ code.\n",
      " |          If None, default seeds in C++ code are used.\n",
      " |      n_jobs : int or None, optional (default=None)\n",
      " |          Number of parallel threads to use for training (can be changed at prediction time by\n",
      " |          passing it as an extra keyword argument).\n",
      " |      \n",
      " |          For better performance, it is recommended to set this to the number of physical cores\n",
      " |          in the CPU.\n",
      " |      \n",
      " |          Negative integers are interpreted as following joblib's formula (n_cpus + 1 + n_jobs), just like\n",
      " |          scikit-learn (so e.g. -1 means using all threads). A value of zero corresponds the default number of\n",
      " |          threads configured for OpenMP in the system. A value of ``None`` (the default) corresponds\n",
      " |          to using the number of physical cores in the system (its correct detection requires\n",
      " |          either the ``joblib`` or the ``psutil`` util libraries to be installed).\n",
      " |      \n",
      " |          .. versionchanged:: 4.0.0\n",
      " |      \n",
      " |      importance_type : str, optional (default='split')\n",
      " |          The type of feature importance to be filled into ``feature_importances_``.\n",
      " |          If 'split', result contains numbers of times the feature is used in a model.\n",
      " |          If 'gain', result contains total gains of splits which use the feature.\n",
      " |      **kwargs\n",
      " |          Other parameters for the model.\n",
      " |          Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n",
      " |      \n",
      " |          .. warning::\n",
      " |      \n",
      " |              \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n",
      " |      \n",
      " |      Note\n",
      " |      ----\n",
      " |      A custom objective function can be provided for the ``objective`` parameter.\n",
      " |      In this case, it should have the signature\n",
      " |      ``objective(y_true, y_pred) -> grad, hess``,\n",
      " |      ``objective(y_true, y_pred, weight) -> grad, hess``\n",
      " |      or ``objective(y_true, y_pred, weight, group) -> grad, hess``:\n",
      " |      \n",
      " |          y_true : numpy 1-D array of shape = [n_samples]\n",
      " |              The target values.\n",
      " |          y_pred : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      " |              The predicted values.\n",
      " |              Predicted values are returned before any transformation,\n",
      " |              e.g. they are raw margin instead of probability of positive class for binary task.\n",
      " |          weight : numpy 1-D array of shape = [n_samples]\n",
      " |              The weight of samples. Weights should be non-negative.\n",
      " |          group : numpy 1-D array\n",
      " |              Group/query data.\n",
      " |              Only used in the learning-to-rank task.\n",
      " |              sum(group) = n_samples.\n",
      " |              For example, if you have a 100-document dataset with ``group = [10, 20, 40, 10, 10, 10]``, that means that you have 6 groups,\n",
      " |              where the first 10 records are in the first group, records 11-30 are in the second group, records 31-70 are in the third group, etc.\n",
      " |          grad : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      " |              The value of the first order derivative (gradient) of the loss\n",
      " |              with respect to the elements of y_pred for each sample point.\n",
      " |          hess : numpy 1-D array of shape = [n_samples] or numpy 2-D array of shape = [n_samples, n_classes] (for multi-class task)\n",
      " |              The value of the second order derivative (Hessian) of the loss\n",
      " |              with respect to the elements of y_pred for each sample point.\n",
      " |      \n",
      " |      For multi-class task, y_pred is a numpy 2-D array of shape = [n_samples, n_classes],\n",
      " |      and grad and hess should be returned in the same format.\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, optional (default=True)\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params: Any) -> 'LGBMModel'\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params\n",
      " |          Parameter names with their new values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from LGBMModel:\n",
      " |  \n",
      " |  best_iteration_\n",
      " |      :obj:`int`: The best iteration of fitted model if ``early_stopping()`` callback has been specified.\n",
      " |  \n",
      " |  best_score_\n",
      " |      :obj:`dict`: The best score of fitted model.\n",
      " |  \n",
      " |  booster_\n",
      " |      Booster: The underlying Booster of this model.\n",
      " |  \n",
      " |  evals_result_\n",
      " |      :obj:`dict`: The evaluation results if validation sets have been specified.\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      :obj:`array` of shape = [n_features]: The feature importances (the higher, the more important).\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          ``importance_type`` attribute is passed to the function\n",
      " |          to configure the type of importance values to be extracted.\n",
      " |  \n",
      " |  feature_name_\n",
      " |      :obj:`list` of shape = [n_features]: The names of features.\n",
      " |  \n",
      " |  n_estimators_\n",
      " |      :obj:`int`: True number of boosting iterations performed.\n",
      " |      \n",
      " |      This might be less than parameter ``n_estimators`` if early stopping was enabled or\n",
      " |      if boosting stopped early due to limits on complexity like ``min_gain_to_split``.\n",
      " |      \n",
      " |      .. versionadded:: 4.0.0\n",
      " |  \n",
      " |  n_features_\n",
      " |      :obj:`int`: The number of features of fitted model.\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      :obj:`int`: The number of features of fitted model.\n",
      " |  \n",
      " |  n_iter_\n",
      " |      :obj:`int`: True number of boosting iterations performed.\n",
      " |      \n",
      " |      This might be less than parameter ``n_estimators`` if early stopping was enabled or\n",
      " |      if boosting stopped early due to limits on complexity like ``min_gain_to_split``.\n",
      " |      \n",
      " |      .. versionadded:: 4.0.0\n",
      " |  \n",
      " |  objective_\n",
      " |      :obj:`str` or :obj:`callable`: The concrete objective used while fitting this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n"
     ]
    }
   ],
   "source": [
    "help(LGBMClassifier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:55:27.811314Z",
     "start_time": "2023-09-28T13:55:27.774684Z"
    }
   },
   "id": "47f6547e582442cb"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "lightGbm = LGBMClassifier(max_depth = 100, learning_rate = 0.01, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:59:44.055901Z",
     "start_time": "2023-09-28T13:59:44.002878Z"
    }
   },
   "id": "e2166e8cd48a5bd9"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 339, number of negative: 152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 386\n",
      "[LightGBM] [Info] Number of data points in the train set: 491, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.690428 -> initscore=0.802120\n",
      "[LightGBM] [Info] Start training from score 0.802120\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": "LGBMClassifier(learning_rate=0.01, max_depth=100, random_state=42)",
      "text/html": "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.01, max_depth=100, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.01, max_depth=100, random_state=42)</pre></div></div></div></div></div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbm.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T14:00:01.645908Z",
     "start_time": "2023-09-28T14:00:01.103957Z"
    }
   },
   "id": "821d048b3d07cf68"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "lightGbm_pred = lightGbm.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T14:00:16.380806Z",
     "start_time": "2023-09-28T14:00:16.348582Z"
    }
   },
   "id": "d2a7f627da40c355"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7804878048780488"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, lightGbm_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T14:00:36.362988Z",
     "start_time": "2023-09-28T14:00:36.326152Z"
    }
   },
   "id": "900de42430ad3e85"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "catBoost = CatBoostClassifier(iterations = 100, learning_rate = 0.01, depth = 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:44:36.859222Z",
     "start_time": "2023-09-28T13:44:36.824830Z"
    }
   },
   "id": "5ba402e45e507192"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6907270\ttotal: 62ms\tremaining: 6.14s\n",
      "1:\tlearn: 0.6884960\ttotal: 66.3ms\tremaining: 3.25s\n",
      "2:\tlearn: 0.6853642\ttotal: 69ms\tremaining: 2.23s\n",
      "3:\tlearn: 0.6832488\ttotal: 74.4ms\tremaining: 1.78s\n",
      "4:\tlearn: 0.6804813\ttotal: 75.5ms\tremaining: 1.44s\n",
      "5:\tlearn: 0.6774902\ttotal: 75.9ms\tremaining: 1.19s\n",
      "6:\tlearn: 0.6753870\ttotal: 80.1ms\tremaining: 1.06s\n",
      "7:\tlearn: 0.6727535\ttotal: 85ms\tremaining: 978ms\n",
      "8:\tlearn: 0.6700002\ttotal: 86ms\tremaining: 869ms\n",
      "9:\tlearn: 0.6674854\ttotal: 89.8ms\tremaining: 808ms\n",
      "10:\tlearn: 0.6643268\ttotal: 90.2ms\tremaining: 730ms\n",
      "11:\tlearn: 0.6625893\ttotal: 95.1ms\tremaining: 697ms\n",
      "12:\tlearn: 0.6597757\ttotal: 95.3ms\tremaining: 638ms\n",
      "13:\tlearn: 0.6572340\ttotal: 95.9ms\tremaining: 589ms\n",
      "14:\tlearn: 0.6555575\ttotal: 99.7ms\tremaining: 565ms\n",
      "15:\tlearn: 0.6532919\ttotal: 101ms\tremaining: 529ms\n",
      "16:\tlearn: 0.6516035\ttotal: 105ms\tremaining: 514ms\n",
      "17:\tlearn: 0.6498225\ttotal: 109ms\tremaining: 498ms\n",
      "18:\tlearn: 0.6480016\ttotal: 112ms\tremaining: 476ms\n",
      "19:\tlearn: 0.6454952\ttotal: 112ms\tremaining: 448ms\n",
      "20:\tlearn: 0.6436606\ttotal: 117ms\tremaining: 438ms\n",
      "21:\tlearn: 0.6417394\ttotal: 119ms\tremaining: 421ms\n",
      "22:\tlearn: 0.6399013\ttotal: 122ms\tremaining: 410ms\n",
      "23:\tlearn: 0.6375117\ttotal: 123ms\tremaining: 389ms\n",
      "24:\tlearn: 0.6356998\ttotal: 127ms\tremaining: 381ms\n",
      "25:\tlearn: 0.6342077\ttotal: 131ms\tremaining: 373ms\n",
      "26:\tlearn: 0.6318551\ttotal: 131ms\tremaining: 355ms\n",
      "27:\tlearn: 0.6296944\ttotal: 135ms\tremaining: 348ms\n",
      "28:\tlearn: 0.6275734\ttotal: 137ms\tremaining: 334ms\n",
      "29:\tlearn: 0.6255462\ttotal: 137ms\tremaining: 321ms\n",
      "30:\tlearn: 0.6239901\ttotal: 141ms\tremaining: 314ms\n",
      "31:\tlearn: 0.6222260\ttotal: 145ms\tremaining: 308ms\n",
      "32:\tlearn: 0.6205842\ttotal: 148ms\tremaining: 301ms\n",
      "33:\tlearn: 0.6189125\ttotal: 152ms\tremaining: 295ms\n",
      "34:\tlearn: 0.6173240\ttotal: 156ms\tremaining: 289ms\n",
      "35:\tlearn: 0.6155227\ttotal: 159ms\tremaining: 283ms\n",
      "36:\tlearn: 0.6137964\ttotal: 163ms\tremaining: 277ms\n",
      "37:\tlearn: 0.6123926\ttotal: 167ms\tremaining: 272ms\n",
      "38:\tlearn: 0.6106964\ttotal: 169ms\tremaining: 264ms\n",
      "39:\tlearn: 0.6089807\ttotal: 172ms\tremaining: 258ms\n",
      "40:\tlearn: 0.6071162\ttotal: 176ms\tremaining: 253ms\n",
      "41:\tlearn: 0.6057725\ttotal: 180ms\tremaining: 248ms\n",
      "42:\tlearn: 0.6046264\ttotal: 183ms\tremaining: 243ms\n",
      "43:\tlearn: 0.6030172\ttotal: 187ms\tremaining: 238ms\n",
      "44:\tlearn: 0.6016363\ttotal: 191ms\tremaining: 233ms\n",
      "45:\tlearn: 0.5996298\ttotal: 191ms\tremaining: 225ms\n",
      "46:\tlearn: 0.5983424\ttotal: 195ms\tremaining: 220ms\n",
      "47:\tlearn: 0.5972832\ttotal: 199ms\tremaining: 216ms\n",
      "48:\tlearn: 0.5960570\ttotal: 203ms\tremaining: 211ms\n",
      "49:\tlearn: 0.5942931\ttotal: 203ms\tremaining: 203ms\n",
      "50:\tlearn: 0.5930901\ttotal: 207ms\tremaining: 199ms\n",
      "51:\tlearn: 0.5912687\ttotal: 207ms\tremaining: 191ms\n",
      "52:\tlearn: 0.5895396\ttotal: 208ms\tremaining: 184ms\n",
      "53:\tlearn: 0.5875287\ttotal: 208ms\tremaining: 177ms\n",
      "54:\tlearn: 0.5862046\ttotal: 210ms\tremaining: 172ms\n",
      "55:\tlearn: 0.5852266\ttotal: 214ms\tremaining: 168ms\n",
      "56:\tlearn: 0.5840453\ttotal: 217ms\tremaining: 164ms\n",
      "57:\tlearn: 0.5828052\ttotal: 221ms\tremaining: 160ms\n",
      "58:\tlearn: 0.5816139\ttotal: 225ms\tremaining: 156ms\n",
      "59:\tlearn: 0.5797491\ttotal: 225ms\tremaining: 150ms\n",
      "60:\tlearn: 0.5781889\ttotal: 226ms\tremaining: 145ms\n",
      "61:\tlearn: 0.5769400\ttotal: 230ms\tremaining: 141ms\n",
      "62:\tlearn: 0.5753688\ttotal: 231ms\tremaining: 135ms\n",
      "63:\tlearn: 0.5740765\ttotal: 234ms\tremaining: 132ms\n",
      "64:\tlearn: 0.5727168\ttotal: 238ms\tremaining: 128ms\n",
      "65:\tlearn: 0.5716616\ttotal: 242ms\tremaining: 124ms\n",
      "66:\tlearn: 0.5702662\ttotal: 246ms\tremaining: 121ms\n",
      "67:\tlearn: 0.5691603\ttotal: 250ms\tremaining: 118ms\n",
      "68:\tlearn: 0.5679764\ttotal: 254ms\tremaining: 114ms\n",
      "69:\tlearn: 0.5668770\ttotal: 259ms\tremaining: 111ms\n",
      "70:\tlearn: 0.5657616\ttotal: 264ms\tremaining: 108ms\n",
      "71:\tlearn: 0.5644202\ttotal: 268ms\tremaining: 104ms\n",
      "72:\tlearn: 0.5634709\ttotal: 273ms\tremaining: 101ms\n",
      "73:\tlearn: 0.5622145\ttotal: 275ms\tremaining: 96.6ms\n",
      "74:\tlearn: 0.5610408\ttotal: 279ms\tremaining: 93.1ms\n",
      "75:\tlearn: 0.5599683\ttotal: 282ms\tremaining: 89ms\n",
      "76:\tlearn: 0.5588693\ttotal: 286ms\tremaining: 85.6ms\n",
      "77:\tlearn: 0.5576698\ttotal: 291ms\tremaining: 82.1ms\n",
      "78:\tlearn: 0.5562147\ttotal: 292ms\tremaining: 77.6ms\n",
      "79:\tlearn: 0.5551744\ttotal: 297ms\tremaining: 74.2ms\n",
      "80:\tlearn: 0.5537236\ttotal: 297ms\tremaining: 69.8ms\n",
      "81:\tlearn: 0.5526630\ttotal: 303ms\tremaining: 66.4ms\n",
      "82:\tlearn: 0.5512782\ttotal: 303ms\tremaining: 62.1ms\n",
      "83:\tlearn: 0.5499768\ttotal: 304ms\tremaining: 57.8ms\n",
      "84:\tlearn: 0.5490405\ttotal: 305ms\tremaining: 53.8ms\n",
      "85:\tlearn: 0.5476458\ttotal: 305ms\tremaining: 49.7ms\n",
      "86:\tlearn: 0.5467232\ttotal: 311ms\tremaining: 46.5ms\n",
      "87:\tlearn: 0.5452874\ttotal: 311ms\tremaining: 42.5ms\n",
      "88:\tlearn: 0.5439227\ttotal: 312ms\tremaining: 38.6ms\n",
      "89:\tlearn: 0.5430359\ttotal: 318ms\tremaining: 35.3ms\n",
      "90:\tlearn: 0.5421878\ttotal: 324ms\tremaining: 32ms\n",
      "91:\tlearn: 0.5414400\ttotal: 330ms\tremaining: 28.7ms\n",
      "92:\tlearn: 0.5405948\ttotal: 336ms\tremaining: 25.3ms\n",
      "93:\tlearn: 0.5397955\ttotal: 343ms\tremaining: 21.9ms\n",
      "94:\tlearn: 0.5386986\ttotal: 344ms\tremaining: 18.1ms\n",
      "95:\tlearn: 0.5376603\ttotal: 351ms\tremaining: 14.6ms\n",
      "96:\tlearn: 0.5364506\ttotal: 352ms\tremaining: 10.9ms\n",
      "97:\tlearn: 0.5354688\ttotal: 354ms\tremaining: 7.22ms\n",
      "98:\tlearn: 0.5344321\ttotal: 360ms\tremaining: 3.63ms\n",
      "99:\tlearn: 0.5335695\ttotal: 366ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": "<catboost.core.CatBoostClassifier at 0x13ac33690>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catBoost.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:44:47.191212Z",
     "start_time": "2023-09-28T13:44:46.791282Z"
    }
   },
   "id": "568963d2f7450680"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "catBoost_pred = catBoost.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:45:17.893358Z",
     "start_time": "2023-09-28T13:45:17.867080Z"
    }
   },
   "id": "113a0524cadc1579"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7804878048780488"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, catBoost_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-28T13:45:27.561185Z",
     "start_time": "2023-09-28T13:45:27.523641Z"
    }
   },
   "id": "70d86f7726c74a16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
